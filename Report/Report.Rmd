---
title: "DATA3888: Predicting Stock Volatility"
author: "Ayush Singh, Tobit Louis, Kylie Haryono, Christy Lee, Zichun Han"
date: "2025-05-24"
format:
  html:
    code-tools: true
    code-fold: false
    fig_caption: yes
    number_sections: true
    embed-resources: true
    theme: cosmo
    css: 
      - https://use.fontawesome.com/releases/v5.0.6/css/all.css
    toc: true
    toc_depth: 4
    toc_float: true
bibliography: references.bib
execute:
  echo: true
  tidy: true
editor: 
  markdown: 
    wrap: 72
---

# Executive Summary

This project explores how well modern deep learning models are able to predict short-term volatility using high-frequency Level-2 Limit Order Book (LOB) data. For trading firms such as Optiver, even minimal gains in forecast accuracy could greatly improve market making and risk control decisions. We compared deep learning models, specifically Transformers and LSTMs, and evaluated them under the same feature pipeline, ensuring predictive accuracy and feasibility both in theory and in practice. The transformer model delivered the strongest predictive performance, creating subtle patterns in noisy financial data. The LSTM model performed consistently across market conditions, offering a reliable balance of accuracy and efficiency. While deep learning models like the Transformer show promise in forecast accuracy, its heavily computational demands and opaque decision process reflect a trade-off between precision and latency. For firms operating in real-time trading settings, this trade-off suggests that LSTMs or even simpler models, if properly tuned, remain a better option.


## Research Question

Short-term volatility forecasting is critical for high-frequency
trading, risk management, and market-making. This study investigates
whether advanced deep learning architectures can enhance predictive
accuracy using ultra-high-frequency Level-2 Limit Order Book (LOB) data.
Specifically, we evaluate whether a Transformer-based model can
outperform traditional Long Short-Term Memory (LSTM) networks when both
models are trained on identically preprocessed datasets with equivalent
engineered features. The goal is to determine if the Transformer's
attention mechanism offers a material advantage in capturing complex
microstructural dynamics inherent in LOB data.

### Hypothesis

#### Null Hypothesis (H₀)

Given equivalent data quality, feature engineering, and preprocessing
conditions, a Transformer-based model does not outperform LSTM networks
in short-term volatility forecasting on Level-2 Limit Order Book (LOB)
data.

#### Alternative Hypothesis (H₁)

Given equivalent data quality, feature engineering, and preprocessing
conditions, a Transformer-based model outperforms LSTM networks in
short-term volatility forecasting on Level-2 Limit Order Book (LOB)
data.

## Key Findings

Our analysis shows that the Transformer model achieves superior
performance in short-term volatility forecasting, consistently
outperforming over 15 candidate models. In general, it attains an
**out-of-sample R² of 0.62** and **QLIKE loss of 0.14**, capturing 62%
of realized volatility variation—substantially higher than the WLS and
Random Forest baselines, and measurably better than the LSTM. In
favorable market regimes, performance improves to **out-of-sample R² of
0.78** and **QLIKE loss of 0.05**. The Transformer's attention mechanism
effectively identifies temporally localized predictive structures in
high-frequency order flow, yielding robust and highly accurate
forecasts.

```{r include-model-plot, echo=FALSE, out.width='100%', fig.cap=""}
knitr::include_graphics("model_comparison.png")
```

## Relevance

The demonstrated predictive accuracy of the Transformer model has direct
implications for high-frequency trading, risk management, and option
pricing. High-quality real-time volatility estimates support tighter
bid-ask spreads, more efficient hedging, and improved capital deployment
under microsecond constraints. By capturing core properties of
stochastic volatility—namely clustering, persistence, and asymmetry—the
Transformer enables materially better real-time decision-making in
trading and portfolio optimisation contexts.

# Volatility Prediction

Volatility is the degree to which the return of an asset deviates from
its expected value over a given time horizon. Formally if $r_t$ denotes
the (logarithmic) return at time $t$ and $\mu = \mathbb{E}[r_t]$ is its
mean, then the (unconditional) volatility $\sigma$ can be defined as the
standard deviation of $r_t$:

$$
\sigma \;=\;\sqrt{\mathbb{E}\bigl[(r_t - \mu)^2\bigr]}\,
$$

In empirical, high‐frequency settings, one often works with realized
volatility over a discrete interval (e.g., within one trading day or a
fixed 10‐minute window). If $\{r_{t,i}\}_{i=1}^N$ are the equally‐spaced
intraday returns within that interval, then the realized volatility is
given by:

$$
\sigma_{\mathrm{realized}} = \sqrt{\sum_{i=1}^N r_{t,i}^2}
$$

which converges, under increasingly fine sampling, to the integrated
variance of the underlying continuous‐time price process.

## Loading Data

The dataset consists of ultra-high-frequency Level-2 limit order book
snapshots recorded at 1-second resolution across 600-second intervals.
Each row corresponds to a fixed-time observation of a single stock and
includes:

-   **Price features:** Best bid and ask prices at levels 1 and 2.
-   **Volume features:** Corresponding sizes at these levels.
-   **Time identifiers:** `time_id` and `seconds_in_bucket`.
-   **Stock identifier:** `stock_id` uniquely indexing each equity.

We load the data from all *112 individual CSV files*, each indexed by
`time_id` and `seconds_in_bucket`. The data is scanned lazily using
**Polars** with strict schema enforcement to ensure memory efficiency.
Files are then consolidated and written as a compressed Parquet object
for downstream processing.


## Filtering

Now, we aim to *predict, rank,* and *cluster* stocks based on the predictability of their short-term (10-minute) volatility. First, raw LOB snapshots are transformed into numerical indicators capturing micro-structure signals. These are then aggregated into 10-minute interval statistics—such as *realised volatility*, *skewness*, *autocorrelation*, and *averaged book-state features*. **Snapshot features** such as microprice ($m_t$)

$$
  m_t = \frac{P_{1,t}^{(b)} \cdot V_{1,t}^{(a)} + P_{1,t}^{(a)} \cdot V_{1,t}^{(b)}}{V_{1,t}^{(b)} + V_{1,t}^{(a)} + \varepsilon}
$$

are computed at each second to serve as a continuous proxy for the “fair” execution price, reflecting order book imbalance at the best level. 

**Aggregate Features** such as log returns ($r_t$)

$$
  r_t = \ln{m_t} - \ln{m_{t-1}} = \ln(m_t/m_{t-1})
$$

and realised volatility ($\mathrm{RV}_{\tau}$)

$$
  \mathrm{RV}_{\tau} 
  = \sqrt{\sum_{t \in \mathcal{T}_\tau} \bigl[\ln(m_t) - \ln(m_{t-1})\bigr]^2}
$$

are also computed, along with skewness, autocorrelation

$$
  \rho_{\tau} 
  = \frac{\displaystyle \sum_{t \in \mathcal{T}_\tau \setminus \{\min\}} (r_t - \bar r)\,(r_{t-1} - \bar r)}
         {\displaystyle \sum_{t \in \mathcal{T}_\tau} (r_t - \bar r)^2}
$$

and tick frequency ($F_{\tau}$)

$$
  F_{\tau} = |\mathcal{T}_\tau|
$$

For each stock, we compute a 13-dimensional vector which represents the average of all these features across time.

To remove extreme outliers, we apply **IQR-based filtering** on each stock’s average realised volatility.

$$
\mu_i = \frac{1}{T_i} \sum_{\tau=1}^{T_i} \mathrm{RV}_{\tau}^{(i)}
$$

This step removes erratic stocks that could distort clustering. The remaining stocks form the sample for **K-means clustering**– where we group stocks with similar micro-structure volatility profiles. By standardising each dimension to unit variance before clustering, we ensure that no single feature dominates the grouping. This approach identifies groups of stocks that cluster into distinct liquidity-volatility regimes based on their microstructural profiles.

In **Stock Selection**, to assess each stock's 10-minute volatility predictability, we fit a lagged linear model using 5 meta-features:

$$
\mathrm{RV}_{\tau}^{(i)} = \gamma_0 + \gamma_1\,z_{\tau-1,1}^{(i)} + \dots + \gamma_5\,z_{\tau-1,5}^{(i)} + \varepsilon_\tau.
$$

where $\gamma_0,\dots,\gamma_5$ are intercept and slope coefficients, and $\varepsilon_\tau$ is the zero-mean residual. We fit this model by **expanding‐window OLS** over $\tau=\tau_{\text{start}},\dots,T_i$, where

$$
\tau_{\text{start}} = \max\{2,\lfloor T_i/2 \rfloor\}
$$

to forecase one-step-ahead values. Performance was evaluated using two metrics:

1.  **Out‐of‐sample** $R^2$ ($R_i^2$): variance explained on holdout intervals
2.  **QLIKE** ($\mathrm{QLIKE}_i$)
    $$
    \mathrm{QLIKE}_i = \frac{1}{N_{\text{oos}}} \sum_{\tau \in \text{oos}} \left(\ln(\widehat{\mathrm{RV}}_{\tau}^{(i)}) + \frac{\mathrm{RV}_{\tau}^{(i)}}{\widehat{\mathrm{RV}}_{\tau}^{(i)}}\right),
    $$ 
    
We combine these 2 metrics into a single score via:
    
$$
S_i = \alpha \cdot R_i^2 - \beta \cdot \mathrm{QLIKE}_i,
$$

with $\alpha = \beta = 0.5$, and select the top $30$ stocks ranked by $S_i$.

In **Exploratory Data Analysis (EDA)**, we inspected raw LOB data for a representative stock by verifying data integrity and computing basic snapshot-level features like midpoints, bid-ask spreads and log-returns. Next, a **grid search** was conducted to set an optimal window size for rolling realised volatility. Building on this, we then engineered more features like said rolling realised volatility and integrated variance. Additionally, we also included lagged returns and squared returns, creating a processing pipeline aligned by `time_id` and `seconds_in_bucket`.


## Feature Engineering

For the top 30 stocks, we enhanced raw LOB data using engineered
features that describe price action, order book structure, and
short-term temporal patterns. These features give downstream models more
signal on liquidity imbalance, market pressure, and volatility.

Along with the *snapshot* and *aggregate* features mentioned in the
filtering section we also calculate:

1.  **Shannon's entropy**

    $$
    s_t = \bigl[V_{1,t}^{(b)},\,V_{2,t}^{(b)},\,V_{1,t}^{(a)},\,V_{2,t}^{(a)}\bigr], 
    \quad
    p_{t,i} = \frac{s_{t,i}}{\sum_{j=1}^{4} s_{t,j}}
    $$

    $$
    H_t = -\sum_{i=1}^{4} p_{t,i} \ln\bigl(p_{t,i}\bigr),
    \quad
    H_t^{Normal} = \frac{H_t}{\ln(4)}
    $$

    This captures the uncertainty in order book volume distribution,
    scaled to $[0,1]$ for compatibility. High entropy indicates balanced
    bid/ask sizes, implying more unpredictable price movements.

2.  **Future Volatility Target**\
    We shift realised volatility 30 seconds ahead, to reserve as
    prediction target. $$
      \text{rv\_future}_t = \text{realised\_volatility}_{t+30}
     $$

3.  **Bipower Variation**\
    This serves as a jump-resistant measure of variance. $$
      \text{bipower\_var}_t 
      = \frac{1}{30} \sum_{j=1}^{30} \bigl|r_{t-j}\bigr|\cdot\bigl|r_{t-j-1}\bigr|
     $$

4.  **Weighted Average Price (WAP) and Its Return**\
    $$
      \text{wap}_t 
      = \frac{P_{1,t}^{(b)}\,V_{1,t}^{(a)} + P_{1,t}^{(a)}\,V_{1,t}^{(b)}}{V_{1,t}^{(b)} + V_{1,t}^{(a)}}
     $$\
    $$
      \text{log\_wap\_return}_t = \ln\bigl(\text{wap}_t / \text{wap}_{t-1}\bigr),
     $$

5.  **Lagged Features**\
    We create first and second lags for imbalance, pressure, and return
    to create short-term memory effects. $$
      x_{t-1},\; x_{t-2},
     $$

6.  **Rolling‐Window Summaries**\
    This captures recent volatility and order book bias.

-   Standard deviation of past 30 log returns:

$$
\text{log\_return}_t^{Rolling} = \sqrt{\frac{1}{29}\sum_{j=1}^{30}(r_{t-j} - \overline{r}_{t-1:t-30})^2}
$$

-   Mean of past 30 imbalance values (short-term directional skew)

7.  **Intrablock Temporal Encoding**
    This encodes relative time position within each 10-min bucket to allow learning of cyclical patterns.

$$
\theta_t = 2\pi \frac{\text{seconds\_in\_bucket}_t}{600},\quad \text{sec\_sin}_t = \sin(\theta_t),\quad \text{sec\_cos}_t = \cos(\theta_t)
$$
        
8.  **Log‐Transformation of Book Sizes**
    This stabilises extreme volumes and reduces skew in raw bid/ask sizes.

$$
\text{size\_log}_t = \ln(1 + \text{size}_t)
$$

    Rows with NaNs or infs are dropped after transformation. Float64s and int64s are cast to 32-bit for memory efficiency.


## Feature Selection

To reduce redundancy and improve training efficiency, we applied two filtering steps.

### 1. Variance Thresholding  
Drops features that are constant across all rows. These add no predictive value and can destabilise models.

### 2. Spearman De-correlation  
Removes highly correlated features based on rank correlations $|\rho_s| > 0.98$.

$$
\rho_s(x,y) = 1 - \frac{6}{n(n^2 - 1)} \sum_{k=1}^n \left(\operatorname{rank}(x_k) - \operatorname{rank}(y_k)\right)^2
$$

When two features are highly monotonic, we drop the one that has higher total correlation with other features.

After filtering, the reattached target column and sorted dataset produce a clean, lower-dimensional matrix for modeling.


## Models

### Weighted Least Squares

To forecast next‐interval realised volatility, we fit a **Weighted Least Squares (WLS)** model using the selected features. WLS accounts for heteroscedasticity by assigning inverse‐variance weights. Let:
- $X \in \mathbb{R}^{N\times p}$ be the predictor matrix (plus intercept),
- $\mathbf{y} \in \mathbb{R}^N$ the target,
- $\mathbf{w} \in \mathbb{R}^N$ the weights, where
$$
w_t = \frac{1}{\operatorname{Var}(y_{t-2000:t}) + \varepsilon}.
$$

WLS minimises:
$$
\min_{\\boldsymbol{\beta}} \sum_{t=1}^N w_t (y_t - \beta_0 - \mathbf{x}_t^\top \boldsymbol{\beta})^2.
$$

We split the data chronologically (80% train, 20% test), add a constant column, and fit `sm.WLS` on the training set. This down‐weights noisy intervals, yielding more stable coefficient estimates under volatility heterogeneity.

As a comparable baseline, WLS produced **Root‐Mean‐Squared Error (RMSE) of 0.000495** on the original scale, **$R^2$ of 0.552**, and **QLIKE loss of 0.2129**.

### Random Forest

To stabilise variance and reduce outlier influence, we log‐transform the target:
$$
y_t^{\text{(log)}} = \ln(1 + \mathrm{rv\_future}_t).
$$

We split data by `time_id`:
- 80% sessions for train + validation, 20% test,
- within the 80%, 90% train, 10% validation,
- features and targets grouped by session to prevent leakage.

We fit the model on $\{X_{\mathrm{train}}, y^{(\text{log})}_{\mathrm{train}}\}$. Key hyperparameters:
- $n_{\text{estimators}} = 500$
- max features per split = $\sqrt{p}$
- min samples per leaf = 3
- no max depth (`max_depth=None`)
- bootstrap enabled, parallel cores used (`n_jobs = -1`)
- random seed = 42

After training, we validate on:
$$
\mathrm{RMSE}_\mathrm{val} = \sqrt{\frac{1}{N_\mathrm{val}} \sum (y_i^{\text{(log)}} - \widehat{y}_i^{\text{(log)}})^2}.
$$

If validation RMSE is too low or high, we tune hyperparameters accordingly. Final evaluation on test set includes:
- Predicting: $\widehat{y}^\text{(log)}_{\mathrm{test}} = \text{rf.predict}(X_{\mathrm{test}})$  
- Inverse transform (if needed): $\exp(\widehat{y}^\text{(log)}) - 1$  
- **RMSE of 0.000458**, **$R^2$ of 0.597**, and **QLIKE of 0.1857**.


### LSTM

We scale all features and the log‐target to $[0,1]$ using MinMaxScaler fit on training data:
$$
x_j^{\text{(scaled)}} = \frac{x_j - \min(x_j)}{\max(x_j) - \min(x_j)}, \quad
y^{\text{(scaled)}} = \frac{\ln(1 + \mathrm{rv\_future}) - \min}{\max - \min}.
$$

Each session is reshaped into 30-second rolling windows. For index $i$, input is $[\mathbf{x}_i, \dots, \mathbf{x}_{i+29}]$, with target at $i+30$. Data is split chronologically:
- 80% sessions = train + validation, 20% = test,
- 90/10 split within train + validation,
- features and targets grouped by session to prevent leakage.

We use a stacked two‐layer LSTM followed by two dense layers to predict
the scaled log‐volatility:
1.  **First LSTM layer (64 units, return_sequences=True)**: allows the second LSTM to capture deeper temporal patterns.
2.  **Dropout (0.2)**: to regularize the model and prevent overfitting to idiosyncratic noise.
3.  **Second LSTM layer (32 units, return_sequences=False)**: summarises past information into a single state vector.
4.  **Dropout (0.2)**
5.  **Dense layer (16 units, ReLU activation)**: introduces a nonlinear transformation to capture cross‐feature interactions.
6.  **Final Dense layer (1 unit, linear)**: outputs a single scalar corresponding to the normalised log‐volatility prediction for the next timestamp.

We compile with **Adam** (lr $10^{-4}$), MSE loss, early stopping (patience = 5), batch size 128, up to 50 epochs.

After predicting on the test set, we invert the scaling.
$$
\widehat{y}^{(\text{log})} = \widehat{y}^{(\text{scaled})} \cdot (\max - \min) + \min, \quad
\widehat{\mathrm{rv\_future}} = \exp(\widehat{y}^{(\text{log})}) - 1.
$$

We then compute:
- **RMSE of 0.000448**,
- **$R^2$ of 0.604**,
- **QLIKE of 0.1821** with clipping to prevent instability.


### Transformer

As with the LSTM, we scale both features and the log‐transformed target to $[0,1]$ using MinMax scaling fit on training data. For each $x_j$:
$$
x_j^{\text{scaled}} = \frac{x_j - \min}{\max - \min}, \quad
y^{\text{(log)}} = \ln(1 + \mathrm{rv\_future}), \quad
y^{\text{scaled}} = \frac{y^{\text{(log)}} - \min}{\max - \min}.
$$

We segment each session into overlapping 30-second windows. If $\mathbf{X}_{1:T} = [\mathbf{x}_1, \dots, \mathbf{x}_T]^\top$, then each input becomes:
$$
[\mathbf{x}_i, \dots, \mathbf{x}_{i+29}], \quad \text{target} = y_{i+30}^{\text{scaled}}.
$$

Sessions are split chronologically: 80% for train+validation (90/10 split), and 20% for test.  

The core model is a Transformer encoder applied to each $30 \times p$ input. With $d_{\text{model}} = 64$, $h = 4$ heads, and $L = 2$ layers, the input is first projected to embedding space:
$$
x^{(1)} = x^{(0)} W_{\text{proj}} + b_{\text{proj}}, \quad W_{\text{proj}} \in \mathbb{R}^{p \times d_{\text{model}}}.
$$

Each Transformer block contains:

1. **Multi‐Head Self Attention**  
   $$
   \text{head}_j = \text{softmax}\left(\frac{Q_j K_j^\top}{\sqrt{d_k}}\right)V_j, \quad
   \text{MHA}(X) = \text{Concat}(\text{head}_1, \dots, \text{head}_h) W_O
   $$

2. **Add & LayerNorm**  
   $$
   x^{(\ell+\tfrac{1}{2})} = \text{LayerNorm}(x^{(\ell)} + \text{MHA}(x^{(\ell)}))
   $$

3. **Feed‐Forward Network (FFN)**  
   $$
   \text{FFN}(z) = \text{ReLU}(z W_1 + b_1) W_2 + b_2
   $$

Final representation is obtained after both layers:
$$
x^{(L)} \in \mathbb{R}^{30 \times d_{\text{model}}}
$$

We apply global average pooling:
$$
\bar{x} = \frac{1}{30} \sum_{t=1}^{30} x_t^{(L)}, \quad
\widehat{y}^{\text{(scaled)}} = \bar{x} w_o + b_o.
$$

We train using the **Adam** optimizer (lr $10^{-3}$) and MSE loss, with early stopping (15 epochs) and batch size 32 for up to 50 epochs.

After training:
$$
\widehat{y}^{\text{(log)}} = \widehat{y}^{\text{(scaled)}} \cdot (\max - \min) + \min, \quad
\widehat{\mathrm{rv\_future}} = \exp(\widehat{y}^{\text{(log)}}) - 1
$$

Finally, we then compute:
- **RMSE of 0.000413**,
- **R-Squared Value ($R^2$) of 0.624**,
- **QLIKE loss of 0.1548** with clipping to prevent instability.


## Evaluation Metrics

To evaluate predictive accuracy, we use three metrics: **R-squared ($R^2$)**, **Mean Squared Error (MSE)**, and **QLIKE Loss**. Each highlights a different aspect of forecast quality.

**R-squared ($R^2$)** measures how much of the variance in realised volatility is explained by the model. Formally:
$$
R^2 = 1 - \frac{\sum_t (y_t - \widehat{y}_t)^2}{\sum_t (y_t - \bar{y})^2},
$$
where $\widehat{y}_t$ is the prediction and $\bar{y}$ the mean of the target. 

Higher $R^2$ implies strong alignment between predicted and actual volatility swings, indicating better overall fit. When computed on the original volatility scale, it evaluates how well forecasts track both low and high volatility regimes; on log‐scaled targets, it emphasises relative (percentage) accuracy, which may reduce sensitivity to extreme outliers.

**MSE** quantifies the average squared difference between predicted and true values:
$$
\mathrm{MSE} = \frac{1}{N} \sum_{t=1}^N (y_t - \widehat{y}_t)^2.
$$

This penalises large errors more than small ones, making it sensitive to outliers. A lower MSE means the model is more accurate on average, though it can be skewed by extreme volatility spikes. While $R^2$ captures relative model fit, MSE gives a direct measure of raw prediction error on the same scale as the target.

The MSE is then converted to RMSE to retain the original scale.

**QLIKE Loss** is designed for evaluating variance forecasts. It penalises the model more when predicted variance is too low—something especially risky in financial settings. It’s defined as:
$$
\text{QLIKE}_t = \log \widehat{y}_t + \frac{y_t}{\widehat{y}_t},
$$
where $y_t$ is the true volatility and $\widehat{y}_t$ is the forecast.

Lower QLIKE values indicate more reliable variance estimation. Unlike MSE, which treats all errors equally, QLIKE focuses on *how wrong* the model is in high-volatility settings—e.g. underestimating sudden spikes. It has become a standard metric in volatility modeling due to this asymmetric penalty.

Together, these three metrics offer complementary views:  
- **$R^2$** reflects how well the model explains overall variability  
- **MSE** captures average prediction error
- **QLIKE** evaluates robustness under financial risk

All scores are computed on the held-out test set using the original volatility scale unless otherwise specified.

## Limitations & Improvements

Although the transformer model yields superior accuracy, its use of a
30‐step context window may overlook longer‐range dependencies; moreover,
the implementation uses only two encoder layers and a modest embedding
size (d_model = 64), which may not capture multi‐scale dynamics in
high‐frequency data. Transformers remain prone to overfitting if the
validation split does not fully represent diverse market regimes, and
the computational burden of self‐attention forced us to train on only a
subset of stocks and files. Mitigating these constraints may require
scaled hardware, model pruning or distillation, or fallback to simpler
architectures.

The LSTM model—configured with two layers of 64 and 32 units plus
dropout—shares similar context‐window limitations and still risks
overfitting without additional regularization or batch normalization;
its hidden‐state dynamics also offer limited interpretability. Random
forests bypass sequence recursion and fix model complexity, but they
ignore temporal order and rely on extensive feature engineering (lags,
rolling statistics, seasonal indicators) to capture autocorrelation.
Capping tree depth at 12 addresses overfitting but may reduce model
flexibility.

Using MinMax scaling presumes that price distributions remain constant
between training and deployment. Because MinMaxScaler is sensitive to
outliers, extreme values can distort feature ranges and destabilize
predictions, indicating that more robust scaling methods or explicit
outlier‐mitigation may be required in live settings.

Although transformer and LSTM models achieve higher standalone accuracy,
their opacity and potential latency can hinder human‐machine
collaboration in trading. Simpler models like WLS or random
forests—though less accurate in isolation—can outperform opaque models
when interpretability supports decision‐making *(Yin and Babic 2024)*.

## Conclusion

This study addresses the practical needs of market makers and trading
firms such as Optiver, where accurate volatility forecasts drive
pricing, hedging, and real‐time inventory management (Optiver 2020). The
Transformer model achieved the lowest MSE and QLIKE and the highest
$R^2$ on test data, indicating its capacity to learn complex volatility
patterns and corroborating evidence that Transformers capture long‐range
dependencies in financial time series (Paul 2024). The LSTM model also
exhibited stable performance across samples, underscoring its robustness
under volatile conditions.

Despite its superior forecasting accuracy, the Transformer’s
computational complexity and limited interpretability may hinder
deployment in latency‐sensitive trading environments. In practice, a
model that balances predictive precision with interpretability and low
latency—such as a well‐tuned LSTM or simpler alternative—can offer firms
like Optiver a more effective and transparent decision‐making framework.

# References

::: {#refs}
:::

## Model Architecture

### LSTM

+---------------------+------------------+-------------+
| **Layer (type)**    | **Output Shape** | **Param #** |
+:====================+:=================+============:+
| lstm_8 (LSTM)       | (None, 30, 64)   | 25,088      |
+---------------------+------------------+-------------+
| dropout_8 (Dropout) | (None, 30, 64)   | 0           |
+---------------------+------------------+-------------+
| lstm_9 (LSTM)       | (None, 32)       | 12,416      |
+---------------------+------------------+-------------+
| dropout_9 (Dropout) | (None, 32)       | 0           |
+---------------------+------------------+-------------+
| dense_8 (Dense)     | (None, 16)       | 528         |
+---------------------+------------------+-------------+
| dense_9 (Dense)     | (None, 1)        | 17          |
+---------------------+------------------+-------------+

:::::: {style="
  font-family: Menlo, 'DejaVu Sans Mono', Consolas, 'Courier New', monospace;
  line-height: 1.4;
"}
<div>

<strong>Total params:</strong> [38,049]{style="color:#00af00;"} (148.63 KB)

</div>

<div>

<strong>Trainable params:</strong> [38,049]{style="color:#00af00;"} (148.63 KB)

</div>

<div>

<strong>Non-trainable params:</strong> [0]{style="color:#00af00;"} (0.00 B)

</div>
::::::

### Transformer

<!-- Colour-coded, full-width model summary with connections -->

+-----------------+-----------------+-------------+-----------------+
| *               | *               | **Param #** | *               |
| *Layer (type)** | *Output Shape** |             | *Connected to** |
+:================+:================+============:+:================+
| input_lay       | (None, 30, 23)  | 0           | –               |
| er (InputLayer) |                 |             |                 |
+-----------------+-----------------+-------------+-----------------+
| dense (Dense)   | (None, 30, 64)  | 1,536       | input_          |
|                 |                 |             | layer\[0\]\[0\] |
+-----------------+-----------------+-------------+-----------------+
| multi_head_     | (None, 30, 64)  | 16,640      | dense\[0\]      |
| attention (Mult |                 |             | \[0\] (q, k, v) |
| iHeadAttention) |                 |             |                 |
+-----------------+-----------------+-------------+-----------------+
| add (Add)       | (None, 30, 64)  | 0           | dense\[         |
|                 |                 |             | 0\]\[0\], multi |
|                 |                 |             | _head_attention |
+-----------------+-----------------+-------------+-----------------+
| layer_norm      | (None, 30, 64)  | 128         | add\[0\]\[0\]   |
| alization (Laye |                 |             |                 |
| rNormalization) |                 |             |                 |
+-----------------+-----------------+-------------+-----------------+
| dense_1 (Dense) | (None, 30, 256) | 16,640      | laye            |
|                 |                 |             | r_normalization |
+-----------------+-----------------+-------------+-----------------+
| dense_2 (Dense) | (None, 30, 64)  | 16,448      | de              |
|                 |                 |             | nse_1\[0\]\[0\] |
+-----------------+-----------------+-------------+-----------------+
| drop            | (None, 30, 64)  | 0           | de              |
| out_1 (Dropout) |                 |             | nse_2\[0\]\[0\] |
+-----------------+-----------------+-------------+-----------------+
| add_1 (Add)     | (None, 30, 64)  | 0           | layer_norm      |
|                 |                 |             | alization, drop |
|                 |                 |             | out_1\[0\]\[0\] |
+-----------------+-----------------+-------------+-----------------+
| layer_normal    | (None, 30, 64)  | 128         | add_1\[0\]\[0\] |
| ization_2 (Laye |                 |             |                 |
| rNormalization) |                 |             |                 |
+-----------------+-----------------+-------------+-----------------+
| multi_head_at   | (None, 30, 64)  | 16,640      | layer_          |
| tention_2 (Mult |                 |             | normalization_2 |
| iHeadAttention) |                 |             |                 |
+-----------------+-----------------+-------------+-----------------+
| add_2 (Add)     | (None, 30, 64)  | 0           | layer_normaliza |
|                 |                 |             | tion_2, multi_h |
|                 |                 |             | ead_attention_2 |
+-----------------+-----------------+-------------+-----------------+
| layer_normal    | (None, 30, 64)  | 128         | add_2\[0\]\[0\] |
| ization_3 (Laye |                 |             |                 |
| rNormalization) |                 |             |                 |
+-----------------+-----------------+-------------+-----------------+
| dense_3 (Dense) | (None, 30, 256) | 16,640      | layer_          |
|                 |                 |             | normalization_3 |
+-----------------+-----------------+-------------+-----------------+
| dense_4 (Dense) | (None, 30, 64)  | 16,448      | de              |
|                 |                 |             | nse_3\[0\]\[0\] |
+-----------------+-----------------+-------------+-----------------+
| drop            | (None, 30, 64)  | 0           | de              |
| out_3 (Dropout) |                 |             | nse_4\[0\]\[0\] |
+-----------------+-----------------+-------------+-----------------+
| add_3 (Add)     | (None, 30, 64)  | 0           | layer_normal    |
|                 |                 |             | ization_3, drop |
|                 |                 |             | out_3\[0\]\[0\] |
+-----------------+-----------------+-------------+-----------------+
| layer_normal    | (None, 30, 64)  | 128         | add_3\[0\]\[0\] |
| ization_4 (Laye |                 |             |                 |
| rNormalization) |                 |             |                 |
+-----------------+-----------------+-------------+-----------------+
| gl              | (None, 64)      | 0           | layer_          |
| obal_average_po |                 |             | normalization_4 |
| oling (GlobalAv |                 |             |                 |
| eragePooling1D) |                 |             |                 |
+-----------------+-----------------+-------------+-----------------+
| drop            | (None, 64)      | 0           | global_         |
| out_4 (Dropout) |                 |             | average_pooling |
+-----------------+-----------------+-------------+-----------------+
| dense_5 (Dense) | (None, 1)       | 65          | drop            |
|                 |                 |             | out_4\[0\]\[0\] |
+-----------------+-----------------+-------------+-----------------+

:::::: {style="
  font-family: Menlo, 'DejaVu Sans Mono', Consolas, 'Courier New', monospace;
  line-height: 1.4;
"}
<div>

<strong>Total params:</strong> [101,569]{style="color:#00af00;"} (396.75 KB)

</div>

<div>

<strong>Trainable params:</strong> [101,569]{style="color:#00af00;"} (396.75 KB)

</div>

<div>

<strong>Non-trainable params:</strong> [0]{style="color:#00af00;"} (0.00 B)

</div>
::::::

## Code

### Imports

```{python, eval=FALSE}
# Core libraries
import os                                   
import random                              
import warnings                           

# Numerical and data handling
import numpy as np                         
import pandas as pd                     
import polars as pl                       

# Visualization
import matplotlib.pyplot as plt           

# File handling
from glob import glob                     

# Preprocessing and feature selection
from sklearn.preprocessing import MinMaxScaler, StandardScaler 
from sklearn.feature_selection import VarianceThreshold, mutual_info_regression  

# Classical models
from sklearn.linear_model import LinearRegression             
from sklearn.ensemble import RandomForestRegressor            

# Unsupervised learning
from sklearn.cluster import KMeans                            

# Evaluation metrics
from sklearn.metrics import r2_score, mean_squared_error, root_mean_squared_error  
from scipy.stats import skew, pearsonr                        

# Statistical modeling
import statsmodels.api as sm                                  

# Deep learning (TensorFlow/Keras)
import tensorflow as tf                                     
from tensorflow import keras                                 
from tensorflow.keras import layers, models, callbacks        
from tensorflow.keras.models import Sequential               
from tensorflow.keras.layers import LSTM, Dense, Dropout      
from tensorflow.keras.callbacks import EarlyStopping
```

### Warnings

```{python, eval=FALSE}
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=DeprecationWarning)
```

### Combining Raw Data

```{python, eval=FALSE}
# Get sorted list of all CSV file paths in the directory
csv_files = sorted(glob("Data/individual_book_train/*.csv"))

# Define column data types
schema = {
    'time_id': pl.Int32,
    'seconds_in_bucket': pl.Int32,
    'bid_price1': pl.Float32,
    'ask_price1': pl.Float32,
    'bid_price2': pl.Float32,
    'ask_price2': pl.Float32,
    'bid_size1': pl.Int32,
    'ask_size1': pl.Int32,
    'bid_size2': pl.Int32,
    'ask_size2': pl.Int32,
    'stock_id': pl.Int32,
}

# Lazily scan CSVs with predefined schema (no type inference)
ldf = pl.scan_csv(
    csv_files,
    schema_overrides=schema,
    infer_schema_length=0  
)

# Load into memory
df = ldf.collect()

# Write to Parquet with Snappy compression
df.write_parquet("Data/112Stocks.parquet", compression="snappy")
```

### Global Parameters

```{python, eval=FALSE}
# reproducibility
RANDOM_STATE = 42
# outlier filtering
VOLATILITY_IQR_MULTIPLIER = 1.5
# clustering
N_CLUSTERS = 5
# modeling threshold
MIN_PERIODS_FOR_MODEL = 10
# metric weights
R2_WEIGHT = 0.5
QLIKE_WEIGHT = 0.5
# numerical stability
EPSILON = 1e-12
# model input length
SEQ_LEN = 30
```

### Helper Functions

```{python, eval=FALSE}
def calculate_basic_features_snapshot(df_slice):
    features = pd.DataFrame(index=df_slice.index)
    # micro price (weighted mid-price)
    features['micro_price'] = (df_slice['bid_price1'] * df_slice['ask_size1'] + \
                               df_slice['ask_price1'] * df_slice['bid_size1']) / \
                              (df_slice['bid_size1'] + df_slice['ask_size1'] + EPSILON)
    # fallback to mid-price if NaN
    features['micro_price'] = features['micro_price'].fillna((df_slice['bid_price1'] + df_slice['ask_price1']) / 2)
    # top-of-book spreads
    features['spread1'] = df_slice['ask_price1'] - df_slice['bid_price1']
    features['spread2'] = df_slice['ask_price2'] - df_slice['bid_price2']
    # size imbalance at level 1
    features['imbalance_size1'] = (df_slice['bid_size1'] - df_slice['ask_size1']) / \
                                  (df_slice['bid_size1'] + df_slice['ask_size1'] + EPSILON)
    # aggregated book pressure (level 1 + 2)
    sum_bid_sizes = df_slice['bid_size1'] + df_slice['bid_size2']
    sum_ask_sizes = df_slice['ask_size1'] + df_slice['ask_size2']
    features['book_pressure'] = sum_bid_sizes / (sum_bid_sizes + sum_ask_sizes + EPSILON)
    return features
```

```{python, eval=FALSE}
def calculate_time_id_features(df_group):
    df_group = df_group.sort_values('seconds_in_bucket').copy()
    snapshot_features = calculate_basic_features_snapshot(df_group)

    # log returns of micro price
    log_returns = np.log(snapshot_features['micro_price'] / snapshot_features['micro_price'].shift(1))
    log_returns = log_returns.replace([np.inf, -np.inf], np.nan).dropna()

    results = {}
    # volatility and skewness
    results['realized_volatility'] = np.std(log_returns) if len(log_returns) > 1 else 0
    results['realized_skewness'] = skew(log_returns) if len(log_returns) > 1 else 0
    # autocorrelation of log returns
    if len(log_returns) > 2:
        ac, _ = pearsonr(log_returns.iloc[1:], log_returns.iloc[:-1])
        results['autocorrelation_log_returns'] = ac if not np.isnan(ac) else 0
    else:
        results['autocorrelation_log_returns'] = 0

    # basic aggregated features
    results['tick_frequency'] = len(df_group)
    results['mean_micro_price'] = snapshot_features['micro_price'].mean()
    results['mean_spread1'] = snapshot_features['spread1'].mean()
    results['mean_spread2'] = snapshot_features['spread2'].mean()
    results['mean_imbalance_size1'] = snapshot_features['imbalance_size1'].mean()
    results['mean_book_pressure'] = snapshot_features['book_pressure'].mean()
    results['mean_bid_size1'] = df_group['bid_size1'].mean()
    results['mean_ask_size1'] = df_group['ask_size1'].mean()
    results['mean_bid_size2'] = df_group['bid_size2'].mean()
    results['mean_ask_size2'] = df_group['ask_size2'].mean()

    return pd.Series(results)
```

```{python, eval=FALSE}
def qlike_loss(y_true, y_pred):
    # avoid division/log(0)
    y_pred = np.maximum(y_pred, EPSILON)
    y_true = np.maximum(y_true, 0)
    valid_indices = (y_true > EPSILON)
    if not np.any(valid_indices):
        return np.nan
    # compute QLIKE loss on valid data
    y_true_f = y_true[valid_indices]
    y_pred_f = y_pred[valid_indices]
    y_pred_f = np.maximum(y_pred_f, EPSILON)
    loss = np.mean(y_true_f / y_pred_f - np.log(y_true_f / y_pred_f) - 1)
    return loss
```

### Feature Generation

```{python, eval=FALSE}
print("Calculating features per stock_id and time_id.")
stock_time_id_features = df.groupby(['stock_id', 'time_id']).apply(calculate_time_id_features).reset_index()
print(f"Calculated detailed features for {stock_time_id_features.shape[0]} stock/time_id pairs.")
print(stock_time_id_features.head())
```

### IQR-Based Filtering

```{python, eval=FALSE}
# compute mean realized volatility per stock
overall_stock_mean_rv = stock_time_id_features.groupby('stock_id')['realized_volatility'].mean().reset_index()
overall_stock_mean_rv = overall_stock_mean_rv.rename(columns={'realized_volatility': 'mean_realized_volatility'})

# define IQR bounds
q1 = overall_stock_mean_rv['mean_realized_volatility'].quantile(0.25)
q3 = overall_stock_mean_rv['mean_realized_volatility'].quantile(0.75)
iqr = q3 - q1
lower_bound = q1 - VOLATILITY_IQR_MULTIPLIER * iqr
upper_bound = q3 + VOLATILITY_IQR_MULTIPLIER * iqr

# filter stocks within bounds and above tiny volatility threshold
epsilon_vol = 1e-7
filtered_stocks_info = overall_stock_mean_rv[
    (overall_stock_mean_rv['mean_realized_volatility'] >= lower_bound) &
    (overall_stock_mean_rv['mean_realized_volatility'] <= upper_bound) &
    (overall_stock_mean_rv['mean_realized_volatility'] > epsilon_vol)
]

# report filtering outcome
n_original_stocks = df['stock_id'].nunique()
n_filtered_stocks = filtered_stocks_info['stock_id'].nunique()
print(f"Original number of stocks: {n_original_stocks}")
print(f"Number of stocks after volatility filtering: {n_filtered_stocks}")

if n_filtered_stocks == 0:
    print("Error: No stocks remaining after filtering. Adjust VOLATILITY_IQR_MULTIPLIER or check data.")
```

### K-means Clustering

```{python, eval=FALSE}
stock_time_id_features_filtered = stock_time_id_features[
    stock_time_id_features['stock_id'].isin(filtered_stocks_info['stock_id'])
]

# selected features for clustering
cluster_feature_cols = [
    'realized_volatility', 'realized_skewness', 'autocorrelation_log_returns', 
    'tick_frequency', 'mean_micro_price', 'mean_spread1', 'mean_spread2', 
    'mean_imbalance_size1', 'mean_book_pressure',
    'mean_bid_size1', 'mean_ask_size1', 'mean_bid_size2', 'mean_ask_size2'
]

# aggregate features at stock level
stock_meta_features_df = stock_time_id_features_filtered.groupby('stock_id')[cluster_feature_cols].mean()

print("Meta-features for clustering (mean of time_id features per stock):")
print(stock_meta_features_df.head())

# clustering stocks based on meta-features
scaler = StandardScaler()
scaled_meta_features = scaler.fit_transform(stock_meta_features_df)

print(f"\nPerforming K-means clustering with K={N_CLUSTERS}...")
kmeans = KMeans(n_clusters=N_CLUSTERS, random_state=RANDOM_STATE, n_init='auto')
stock_meta_features_df['cluster'] = kmeans.fit_predict(scaled_meta_features)

print("Clustering results (stock_id and assigned cluster):")
print(stock_meta_features_df[['cluster']].head())
```

### Individual R² and QLIKE evaluation

```{python, eval=FALSE}
r_squared_feature_cols = [
    'realized_volatility', 'mean_spread1', 'mean_imbalance_size1', 
    'mean_book_pressure', 'mean_micro_price'
]
stock_scores_list = []
stock_time_id_features_filtered = stock_time_id_features_filtered.sort_values(['stock_id', 'time_id'])

for stock_id in filtered_stocks_info['stock_id']:
    stock_data = stock_time_id_features_filtered[stock_time_id_features_filtered['stock_id'] == stock_id].copy()
    
    if len(stock_data) < MIN_PERIODS_FOR_MODEL:
        print(f"Stock {stock_id}: Insufficient data ({len(stock_data)} periods) for R2/QLIKE, skipping.")
        stock_scores_list.append({'stock_id': stock_id, 'r_squared': np.nan, 'qlike': np.nan})
        continue

    for col in r_squared_feature_cols:
        stock_data[f'prev_{col}'] = stock_data[col].shift(1)
    
    stock_data = stock_data.dropna() 

    if len(stock_data) < 2: 
        print(f"Stock {stock_id}: Insufficient data after lagging for R2/QLIKE, skipping.")
        stock_scores_list.append({'stock_id': stock_id, 'r_squared': np.nan, 'qlike': np.nan})
        continue

    y_true_r2_all = []
    y_pred_r2_all = []
    y_true_qlike_all = []
    y_pred_qlike_all = []

    start_prediction_idx = max(2, MIN_PERIODS_FOR_MODEL // 2)


    for i in range(start_prediction_idx, len(stock_data)):
        train_df = stock_data.iloc[:i]
        current_period_data = stock_data.iloc[i]

        X_train = train_df[[f'prev_{col}' for col in r_squared_feature_cols]]
        y_train = train_df['realized_volatility']
        
        X_current = pd.DataFrame(current_period_data[[f'prev_{col}' for col in r_squared_feature_cols]]).T
        y_current_true_r2 = current_period_data['realized_volatility']

        if len(X_train) >= 2: 
            try:
                model = LinearRegression()
                model.fit(X_train, y_train)
                y_current_pred_r2 = model.predict(X_current)[0]
                
                y_true_r2_all.append(y_current_true_r2)
                y_pred_r2_all.append(y_current_pred_r2)
            except Exception:
                pass

        historical_rv_for_qlike = train_df['realized_volatility']
        if not historical_rv_for_qlike.empty:
            forecast_rv_qlike = historical_rv_for_qlike.mean()
            y_current_true_qlike = current_period_data['realized_volatility']

            y_true_qlike_all.append(y_current_true_qlike)
            y_pred_qlike_all.append(forecast_rv_qlike)

    r_squared_stock = np.nan
    if len(y_true_r2_all) >= 2 and len(set(y_true_r2_all)) > 1: 
        r_squared_stock = r2_score(y_true_r2_all, y_pred_r2_all)
    
    qlike_stock = np.nan
    if y_true_qlike_all:
        qlike_stock = qlike_loss(np.array(y_true_qlike_all), np.array(y_pred_qlike_all))

    stock_scores_list.append({
        'stock_id': stock_id,
        'r_squared': r_squared_stock,
        'qlike': qlike_stock
    })
    print(f"Stock {stock_id}: R^2 = {r_squared_stock:.4f}, QLIKE = {qlike_stock:.4f} (from {len(y_true_r2_all)} R2 points, {len(y_true_qlike_all)} QLIKE points)")
```

```{python, eval=FALSE}
stock_scores_df = pd.DataFrame(stock_scores_list)
stock_scores_df = pd.merge(stock_scores_df, stock_meta_features_df[['cluster']].reset_index(), on='stock_id', how='left')
print("\nCalculated R-squared and QLIKE scores:")
print(stock_scores_df.head())
```

```{python, eval=FALSE}
# remove incomplete results
stock_scores_df = stock_scores_df.dropna(subset=['r_squared', 'qlike'])

if stock_scores_df.empty:
    print("Error: No stocks remaining after calculating R-squared/QLIKE (all NaNs or empty). Check calculation steps or MIN_PERIODS_FOR_MODEL.")
    exit()
  
# combine scores using weighted formula  
stock_scores_df['combined_score'] = R2_WEIGHT * stock_scores_df['r_squared'] - \
                                    QLIKE_WEIGHT * stock_scores_df['qlike']
# rank and select top stocks
top_stocks = stock_scores_df.sort_values(by='combined_score', ascending=False)

print(f"\nTop 30 stocks based on combined score ({R2_WEIGHT}*R^2 - {QLIKE_WEIGHT}*QLIKE):")
N_TOP_STOCKS = 30
final_selection = top_stocks.head(N_TOP_STOCKS)
print(final_selection)
```

### Saving Selected Stocks

```{python, eval=FALSE}
# For Reference only, choose from final_selection in practice. 
selected_stock_ids = [1, 5, 7, 8, 22, 27, 32, 44, 50, 55, 
                      59, 62, 63, 73, 75, 76, 78, 80, 81, 84, 
                      85, 86, 89, 96, 97, 101, 102, 109, 115, 120]

df = pd.read_parquet("Data/112Stocks.parquet")
df = df[df["stock_id"].isin(selected_stock_ids)]
df.to_parquet("Data/30Stocks.parquet")
```

### Feature Engineering

```{python, eval=FALSE}
# Only for top 30 stocks
df = pd.read_parquet("Data/30stocks.parquet")

def make_features(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()

    df['mid_price'] = (df['bid_price1'] + df['ask_price1']) / 2
    df['spread']    = df['ask_price1'] - df['bid_price1']
    
    with np.errstate(divide='ignore', invalid='ignore'):
        num  = df['bid_size1'] - df['ask_size1']
        den  = df['bid_size1'] + df['ask_size1']
        df['imbalance'] = np.where(den > 0, num / den, np.nan)

        num2 = (df['bid_size1'] + df['bid_size2']) - (df['ask_size1'] + df['ask_size2'])
        den2 = df[['bid_size1','bid_size2','ask_size1','ask_size2']].sum(axis=1)
        df['book_pressure'] = np.where(den2 > 0, num2 / den2, np.nan)

    df['normalized_spread'] = df['spread'] / df['mid_price'].replace(0, np.nan)
    df['OBI_L2'] = np.where(den2 > 0, (df['bid_size1'] + df['bid_size2']) / den2, np.nan)

    sizes = df[['bid_size1','bid_size2','ask_size1','ask_size2']].astype(float).values
    total = sizes.sum(axis=1, keepdims=True)
    p = np.divide(sizes, total, where=total != 0)
    entropy = -np.nansum(np.where(p > 0, p * np.log(p), 0), axis=1)
    df['LOB_entropy'] = entropy
    df['LOB_entropy_normalized'] = entropy / np.log(4)

    df['log_return'] = (
        df.groupby('time_id')['mid_price']
          .transform(lambda x: np.log(x / x.shift(1)))
    )

    df['realized_volatility'] = (
        df.groupby('time_id')['log_return']
        .transform(lambda x: np.sqrt(
            ((x.shift(1) ** 2)
                .rolling(30, min_periods=1)
                .sum()
            ).clip(lower=0)
        ))
    )

    df['rv_future'] = (
        df.groupby('time_id')['realized_volatility'].shift(-30)   
    )

    df['bipower_var'] = (
        df.groupby('time_id')['log_return']
          .transform(lambda x: x.abs().shift(1)
                       .rolling(2, min_periods=1)
                       .apply(lambda r: r[0] * r[1], raw=True)
                       .rolling(30, min_periods=1)
                       .mean())
    )

    df['wap'] = (
        (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) /
        (df['bid_size1'] + df['ask_size1']).replace(0, np.nan)
    )
    
    df['log_wap_return'] = (
        df.groupby('time_id')['wap']
          .transform(lambda x: np.log(x / x.shift(1)))
    )

    for col in ['imbalance', 'book_pressure', 'log_return']:
        df[f'{col}_lag1'] = df.groupby('time_id')[col].shift(1)
        df[f'{col}_lag2'] = df.groupby('time_id')[col].shift(2)

    df['rolling_vol_30'] = (
        df.groupby('time_id')['log_return']
          .transform(lambda x: x.shift(1).rolling(30, min_periods=1).std())
    )
    df['rolling_imbalance_mean_30'] = (
        df.groupby('time_id')['imbalance']
          .transform(lambda x: x.shift(1).rolling(30, min_periods=1).mean())
    )

    df = df.dropna()   
    df = df.replace([np.inf, -np.inf], np.nan)

    theta = 2 * np.pi * df['seconds_in_bucket'] / 600 # period = 600
    df['sec_sin'] = np.sin(theta)
    df['sec_cos'] = np.cos(theta)

    for c in ['bid_size1','ask_size1','bid_size2','ask_size2']:
        df[c + '_log'] = np.log1p(df[c])
        df.drop(columns=c, inplace=True)

    return df
  
df = make_features(df)
```

### Type Conversion

```{python, eval=FALSE}
df = df.astype({col: 'float32' if df[col].dtype == 'float64' else 'int32' 
                for col in df.columns 
                if df[col].dtype in ['float64', 'int64']})
```

### Variance Thresholding

```{python, eval=FALSE}
X = df.drop(columns=['rv_future'])
```

```{python, eval=FALSE}
selector = VarianceThreshold(threshold=0.0)
X_reduced = selector.fit_transform(X)
selected_columns = X.columns[selector.get_support()]
X_reduced_df = pd.DataFrame(X_reduced, columns=selected_columns, index=X.index)
```

```{python, eval=FALSE}
dfR = X_reduced_df.astype({col: 'float32' if X_reduced_df[col].dtype == 'float64' else 'int32' 
                for col in X_reduced_df.columns 
                if X_reduced_df[col].dtype in ['float64', 'int64']})
```

### Spearman Correlation

```{python, eval=FALSE}
corr = dfR.corr(method='spearman').abs()
to_drop = {c for c in corr.columns for r in corr.columns
if r != c and corr.loc[r, c] > .98 and corr.loc[r].sum() < corr.loc[c].sum()}
to_drop
```

```{python, eval=FALSE}
dfR = dfR.drop(columns=list(to_drop))

dfR['rv_future'] = df['rv_future']

dfR = dfR.sort_values(
    ['stock_id', 'time_id', 'seconds_in_bucket'],
    ascending=[True, True, True]
).reset_index(drop=True)

dfR.to_parquet("Data/FE30Stocks.parquet")
```

### Weighted Least Square

```{python, eval=FALSE}
def qlike_loss(actual, pred, eps=1e-12):
    a = np.clip(actual, eps, None)     
    f = np.clip(pred,   eps, None)
    r = a / f
    return np.mean(r - np.log(r) - 1.0) 
```

```{python, eval=FALSE}
# Feature and target column names
feature_cols = ['stock_id','mid_price', 'spread', 'imbalance',
       'book_pressure', 'LOB_entropy', 'log_return', 'bipower_var',
       'log_wap_return', 'imbalance_lag1', 'imbalance_lag2',
       'book_pressure_lag1', 'book_pressure_lag2', 'log_return_lag1',
       'log_return_lag2', 'rolling_vol_30', 'rolling_imbalance_mean_30',
       'sec_sin', 'sec_cos', 'bid_size1_log', 'ask_size1_log', 'bid_size2_log',
       'ask_size2_log']
target_col = 'rv_future'
```

```{python, eval=FALSE}
# Load data
df = pd.read_parquet("DATA3888/Optiver-07/Data/FE30Stocks.parquet")
```

```{python, eval=FALSE}
# Prepare features, target, and weights (inverse variance as heteroscedastic weights)
X = df[feature_cols].astype('float32')
y = df[target_col].astype('float32')
w = 1.0 / (y.rolling(2000, min_periods=1).var().fillna(y.var()))
```

```{python, eval=FALSE}
# Train-test split
split_idx = int(len(df) * 0.8)        
X_train, X_test = X.iloc[:split_idx],  X.iloc[split_idx:]
y_train, y_test = y.iloc[:split_idx],  y.iloc[split_idx:]
w_train, w_test = w.iloc[:split_idx],  w.iloc[split_idx:]
```

```{python, eval=FALSE}
# Add intercept term
X_train_c = sm.add_constant(X_train, has_constant='add')
X_test_c  = sm.add_constant(X_test,  has_constant='add')

# Weighted Least Squares regression
model     = sm.WLS(y_train, X_train_c, weights=w_train)
results   = model.fit()
print(results.summary())
```

```{python, eval=FALSE}
# Predict and evaluate
y_pred = results.predict(X_test_c)
r2     = r2_score(y_test, y_pred)
qlike  = qlike_loss(y_test.values, y_pred)

print(f"Out-of-sample R²   : {r2:0.4f}")
print(f"Out-of-sample QLIKE: {qlike:0.6f}")
```

### Random Forest

```{python, eval=FALSE}
df = pd.read_parquet("DATA3888/Optiver-07/Data/FE30Stocks.parquet")

feature_cols_mod = ['stock_id', 'mid_price', 'spread', 'imbalance',
       'book_pressure', 'LOB_entropy', 'log_return', 'bipower_var',
       'log_wap_return', 'imbalance_lag1', 'imbalance_lag2',
       'book_pressure_lag1', 'book_pressure_lag2', 'log_return_lag1',
       'log_return_lag2', 'rolling_vol_30', 'rolling_imbalance_mean_30',
       'sec_sin', 'sec_cos', 'bid_size1_log', 'ask_size1_log', 'bid_size2_log',
       'ask_size2_log']
target_col = "rv_future"

df['rv_future_log'] = np.log1p(df[target_col])
target_col_mod   = 'rv_future_log'
```

```{python, eval=FALSE}
unique_sessions = np.sort(df['time_id'].unique())
split_idx       = int(len(unique_sessions) * 0.8)

train_val_sessions = unique_sessions[:split_idx]
test_sessions      = unique_sessions[split_idx:]

train_val_df = df[df['time_id'].isin(train_val_sessions)].copy()
test_df      = df[df['time_id'].isin(test_sessions)].copy()

val_cut        = int(len(train_val_sessions) * 0.9)
train_sessions = train_val_sessions[:val_cut]
val_sessions   = train_val_sessions[val_cut:]
```

```{python, eval=FALSE}
train_df = train_val_df[train_val_df['time_id'].isin(train_sessions)]
val_df   = train_val_df[train_val_df['time_id'].isin(val_sessions)]

X_train = train_df[feature_cols_mod].values
y_train = train_df[target_col_mod].values.ravel()

X_val   = val_df[feature_cols_mod].values
y_val   = val_df[target_col_mod].values.ravel()

X_test  = test_df[feature_cols_mod].values
y_test  = test_df[target_col_mod].values.ravel()
```

```{python, eval=FALSE}
rf = RandomForestRegressor(
    n_estimators=500,
    max_depth=None,
    max_features='sqrt',
    min_samples_leaf=3,
    bootstrap=True,
    n_jobs=-1,
    random_state=42,
    verbose=1
)
rf.fit(X_train, y_train)
```

```{python, eval=FALSE}
val_pred  = rf.predict(X_val)
val_rmse  = root_mean_squared_error(y_val, val_pred)
print(f"Validation RMSE: {val_rmse:.6f}")
```

```{python, eval=FALSE}
pred = rf.predict(X_test)
rmse = root_mean_squared_error(y_test, pred)
print(f"Out-of-sample RMSE = {rmse:.6f}")
```

```{python, eval=FALSE}
y_true_raw = y_test                  
y_pred_raw = rf.predict(X_test)     

rmse = root_mean_squared_error(y_true_raw, y_pred_raw)
r2 = r2_score(y_true_raw, y_pred_raw)

def qlike_safe(actual, forecast, eps=1e-8):
    a = np.clip(actual, eps, None)
    f = np.clip(forecast, eps, None)
    r = a / f
    return np.mean(r - np.log(r) - 1.0)

ql = qlike_safe(y_true_raw, y_pred_raw)

print(f"Out‑of‑sample RMSE: {rmse:.6f}")
print(f"R² score         : {r2:.6f}")
print(f"QLIKE            : {ql:.6f}")
```

### LSTM

```{python, eval=FALSE}
random.seed(3888)
np.random.seed(3888)
tf.random.set_seed(3888)
```

```{python, eval=FALSE}
df = pd.read_parquet("DATA3888/Optiver-07/Data/FE30Stocks.parquet")

feature_cols = ['mid_price', 'spread',
       'imbalance', 'book_pressure', 'LOB_entropy', 'log_return',
       'bipower_var', 'log_wap_return', 'imbalance_lag1', 'imbalance_lag2',
       'book_pressure_lag1', 'book_pressure_lag2', 'log_return_lag1',
       'log_return_lag2', 'rolling_vol_30', 'rolling_imbalance_mean_30',
       'sec_sin', 'sec_cos', 'bid_size1_log', 'ask_size1_log', 'bid_size2_log',
       'ask_size2_log']
target_col = "rv_future"

df['rv_future_log'] = np.log1p(df['rv_future'])
feature_cols_mod = [c for c in feature_cols if c!='rv_future'] 
target_col_mod     = 'rv_future_log'
```

```{python, eval=FALSE}
unique_sessions = df["time_id"].sort_values().unique()
split_idx       = int(len(unique_sessions) * 0.8)          
train_sessions  = unique_sessions[:split_idx]
test_sessions   = unique_sessions[split_idx:]
```

```{python, eval=FALSE}
train_df = df[df["time_id"].isin(train_sessions)].copy()
test_df  = df[df["time_id"].isin(test_sessions)].copy()
```

```{python, eval=FALSE}
x_scaler = MinMaxScaler().fit(train_df[feature_cols_mod])
y_scaler = MinMaxScaler(feature_range=(0,1)).fit(train_df[[target_col_mod]])

train_df[feature_cols_mod] = x_scaler.transform(train_df[feature_cols_mod])
test_df[feature_cols_mod]  = x_scaler.transform(test_df[feature_cols_mod])
train_df[target_col_mod]   = y_scaler.transform(train_df[[target_col_mod]])
test_df[target_col_mod]    = y_scaler.transform(test_df[[target_col_mod]])
```

```{python, eval=FALSE}
def build_sequences(df_part: pd.DataFrame, feature_cols, target_col, seq_len):
    X, y = [], []
    for _, session in df_part.groupby("time_id"):
        data   = session[feature_cols].values
        target = session[target_col].values
        for i in range(len(session) - seq_len):
            X.append(data[i : i + seq_len])
            y.append(target[i + seq_len])
    return np.asarray(X), np.asarray(y)

X_train, y_train = build_sequences(train_df, feature_cols, target_col, SEQ_LEN)
X_test,  y_test  = build_sequences(test_df,  feature_cols, target_col, SEQ_LEN)
```

```{python, eval=FALSE}
val_split_idx = int(len(train_sessions) * 0.9)
val_sessions  = train_sessions[val_split_idx:]
train_sessions= train_sessions[:val_split_idx]

val_df = train_df[train_df["time_id"].isin(val_sessions)]
train_df = train_df[train_df["time_id"].isin(train_sessions)]

X_train, y_train = build_sequences(train_df, feature_cols_mod, 'rv_future_log', SEQ_LEN)
X_val,   y_val   = build_sequences(val_df,   feature_cols_mod, 'rv_future_log', SEQ_LEN)
X_test,  y_test  = build_sequences(test_df,  feature_cols_mod, 'rv_future_log', SEQ_LEN)
```

```{python, eval=FALSE}
def build_lstm_model(seq_len, num_features):
    model = Sequential()
    model.add(LSTM(64, return_sequences=True, input_shape=(seq_len, num_features)))
    model.add(Dropout(0.2))
    model.add(LSTM(32))
    model.add(Dropout(0.2))
    model.add(Dense(16, activation='relu'))
    model.add(Dense(1))  
    return model

NUM_FEATURES = X_train.shape[2]
model = build_lstm_model(SEQ_LEN, NUM_FEATURES)

callbacks = [
    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
]

model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),
    loss='mse',
    metrics=['mae', 'mse']
)
model.summary()
```

```{python, eval=FALSE}
history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=50, 
    batch_size=128,
    callbacks=callbacks,
    verbose=1
)
```

```{python, eval=FALSE}
pred_scaled = model.predict(X_test, verbose=1).flatten()
actual_scaled = y_test.flatten()
predictions = y_scaler.inverse_transform(pred_scaled.reshape(-1, 1)).flatten()
actuals     = y_scaler.inverse_transform(actual_scaled.reshape(-1, 1)).flatten()
```

```{python, eval=FALSE}
ql = qlike_loss(actuals, predictions)
mse   = np.mean((predictions - actuals) ** 2)
rmse  = np.sqrt(mse)
r2 = r2_score(actuals, predictions)
print(f"Test RMSE (volatility): {rmse:.9f}")
print(f"R² score (σ prediction): {r2:.6f}")
print("QLIKE:", ql)
```

### Transformer

```{python, eval=FALSE}
df = pd.read_parquet("DATA3888/Optiver-07/Data/FE30Stocks.parquet")

feature_cols = ['mid_price', 'spread',
       'imbalance', 'book_pressure', 'LOB_entropy', 'log_return',
       'bipower_var', 'log_wap_return', 'imbalance_lag1', 'imbalance_lag2',
       'book_pressure_lag1', 'book_pressure_lag2', 'log_return_lag1',
       'log_return_lag2', 'rolling_vol_30', 'rolling_imbalance_mean_30',
       'sec_sin', 'sec_cos', 'bid_size1_log', 'ask_size1_log', 'bid_size2_log',
       'ask_size2_log']
target_col = "rv_future"

df['rv_future_log'] = np.log1p(df['rv_future'])
feature_cols_mod = [c for c in feature_cols if c!='rv_future'] 
target_col_mod     = 'rv_future_log'
```

```{python, eval=FALSE}
unique_sessions = df["time_id"].sort_values().unique()
split_idx       = int(len(unique_sessions) * 0.8)          
train_sessions  = unique_sessions[:split_idx]
test_sessions   = unique_sessions[split_idx:]
```

```{python, eval=FALSE}
train_df = df[df["time_id"].isin(train_sessions)].copy()
test_df  = df[df["time_id"].isin(test_sessions)].copy()
```

```{python, eval=FALSE}
x_scaler = MinMaxScaler().fit(train_df[feature_cols_mod])
y_scaler = MinMaxScaler(feature_range=(0,1)).fit(train_df[[target_col_mod]])

train_df[feature_cols_mod] = x_scaler.transform(train_df[feature_cols_mod])
test_df[feature_cols_mod]  = x_scaler.transform(test_df[feature_cols_mod])
train_df[target_col_mod]   = y_scaler.transform(train_df[[target_col_mod]])
test_df[target_col_mod]    = y_scaler.transform(test_df[[target_col_mod]])
```

```{python, eval=FALSE}
X_train, y_train = build_sequences(train_df, feature_cols, target_col, SEQ_LEN)
X_test,  y_test  = build_sequences(test_df,  feature_cols, target_col, SEQ_LEN)
```

```{python, eval=FALSE}
def build_transformer_model(seq_len, num_features, d_model=64, num_heads=4, num_layers=2):
    inputs = layers.Input(shape=(seq_len, num_features))
    x = layers.Dense(d_model)(inputs)
    for _ in range(num_layers):
        attn_output = layers.MultiHeadAttention(
            num_heads=num_heads, key_dim=d_model)(x, x)
        x = layers.Add()([x, attn_output])
        x = layers.LayerNormalization(epsilon=1e-6)(x)
        ffn_out = layers.Dense(d_model * 4, activation="relu")(x)
        ffn_out = layers.Dense(d_model)(ffn_out)
        x       = layers.Add()([x, ffn_out])
        x       = layers.LayerNormalization(epsilon=1e-6)(x)
    x = layers.GlobalAveragePooling1D()(x)  
    output = layers.Dense(1)(x)
    return models.Model(inputs, output)

model = build_transformer_model(SEQ_LEN, len(feature_cols))
model.compile(optimizer=tf.keras.optimizers.Adam(1e-3), loss="mse")
model.summary()
```

```{python, eval=FALSE}
val_split_idx = int(len(train_sessions) * 0.9)
val_sessions  = train_sessions[val_split_idx:]
train_sessions= train_sessions[:val_split_idx]

val_df = train_df[train_df["time_id"].isin(val_sessions)]
train_df = train_df[train_df["time_id"].isin(train_sessions)]

X_train, y_train = build_sequences(train_df, feature_cols_mod, 'rv_future_log', SEQ_LEN)
X_val,   y_val   = build_sequences(val_df,   feature_cols_mod, 'rv_future_log', SEQ_LEN)
X_test,  y_test  = build_sequences(test_df,  feature_cols_mod, 'rv_future_log', SEQ_LEN)
```

```{python, eval=FALSE}
num_feats = X_train.shape[2]  
model = build_transformer_model(SEQ_LEN, num_feats)

model.compile(
    optimizer=tf.keras.optimizers.Adam(1e-3),
    loss="mse"
)

history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=50,
    batch_size=32,
    callbacks=[callbacks.EarlyStopping(
        monitor="val_loss", patience=15, restore_best_weights=True
    )],
    verbose=1,
)
```

```{python, eval=FALSE}
pred_scaled = model.predict(X_test, verbose=1).flatten()
actual_scaled = y_test.flatten()
predictions = y_scaler.inverse_transform(pred_scaled.reshape(-1, 1)).flatten()
actuals     = y_scaler.inverse_transform(actual_scaled.reshape(-1, 1)).flatten()
mse   = np.mean((predictions - actuals) ** 2)
rmse  = np.sqrt(mse)
print(f"Test RMSE (volatility): {rmse:.9f}")
```

```{python, eval=FALSE}
r2 = r2_score(actuals, predictions)
print(f"R² score (σ prediction) = {r2:.6f}")
```

```{python, eval=FALSE}
def qlike_safe(actual, forecast, eps=1e-12):
    a = np.clip(actual,   eps, None)
    f = np.clip(forecast, eps, None)
    r = a / f
    return np.mean(r - np.log(r) - 1.0)

ql = qlike_safe(actuals, predictions)
print("QLIKE:", ql)
```

## Plots

```{r include-corr-plot, echo=FALSE, out.width='100%', fig.cap=""}
knitr::include_graphics("Corr-1.png")
```

```{r include-hist-plot, echo=FALSE, out.width='100%', fig.cap=""}
knitr::include_graphics("FinalDF-Hist.png")
```

```{r include-rvgs-plot, echo=FALSE, out.width='100%', fig.cap="Data Distribution"}
knitr::include_graphics("RV-GS.png")
```
