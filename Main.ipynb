{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DATA3888 Project: Optiver**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold, mutual_info_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import r2_score, mean_squared_error, root_mean_squared_error\n",
    "from scipy.stats import skew, pearsonr\n",
    "import statsmodels.api as sm\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Combining CSVs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = sorted(glob(\"Data/individual_book_train/*.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "    'time_id': pl.Int32,\n",
    "    'seconds_in_bucket': pl.Int32,\n",
    "    'bid_price1': pl.Float32,\n",
    "    'ask_price1': pl.Float32,\n",
    "    'bid_price2': pl.Float32,\n",
    "    'ask_price2': pl.Float32,\n",
    "    'bid_size1': pl.Int32,\n",
    "    'ask_size1': pl.Int32,\n",
    "    'bid_size2': pl.Int32,\n",
    "    'ask_size2': pl.Int32,\n",
    "    'stock_id': pl.Int32,\n",
    "}\n",
    "\n",
    "ldf = pl.scan_csv(\n",
    "    csv_files,\n",
    "    schema_overrides=schema,\n",
    "    infer_schema_length=0  \n",
    ")\n",
    "\n",
    "\n",
    "df = ldf.collect()\n",
    "df.write_parquet(\"Data/112Stocks.parquet\", compression=\"snappy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"Data/112Stocks.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 167253289 entries, 0 to 167253288\n",
      "Data columns (total 11 columns):\n",
      " #   Column             Dtype  \n",
      "---  ------             -----  \n",
      " 0   time_id            int32  \n",
      " 1   seconds_in_bucket  int32  \n",
      " 2   bid_price1         float32\n",
      " 3   ask_price1         float32\n",
      " 4   bid_price2         float32\n",
      " 5   ask_price2         float32\n",
      " 6   bid_size1          int32  \n",
      " 7   ask_size1          int32  \n",
      " 8   bid_size2          int32  \n",
      " 9   ask_size2          int32  \n",
      " 10  stock_id           int32  \n",
      "dtypes: float32(4), int32(7)\n",
      "memory usage: 6.9 GB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "VOLATILITY_IQR_MULTIPLIER = 1.5\n",
    "N_CLUSTERS = 5 \n",
    "MIN_PERIODS_FOR_MODEL = 10 \n",
    "R2_WEIGHT = 0.5\n",
    "QLIKE_WEIGHT = 0.5\n",
    "EPSILON = 1e-12\n",
    "SEQ_LEN = 30  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_basic_features_snapshot(df_slice):\n",
    "    features = pd.DataFrame(index=df_slice.index)\n",
    "    features['micro_price'] = (df_slice['bid_price1'] * df_slice['ask_size1'] + \\\n",
    "                               df_slice['ask_price1'] * df_slice['bid_size1']) / \\\n",
    "                              (df_slice['bid_size1'] + df_slice['ask_size1'] + EPSILON)\n",
    "    features['micro_price'] = features['micro_price'].fillna((df_slice['bid_price1'] + df_slice['ask_price1']) / 2)\n",
    "    features['spread1'] = df_slice['ask_price1'] - df_slice['bid_price1']\n",
    "    features['spread2'] = df_slice['ask_price2'] - df_slice['bid_price2']\n",
    "    features['imbalance_size1'] = (df_slice['bid_size1'] - df_slice['ask_size1']) / \\\n",
    "                                  (df_slice['bid_size1'] + df_slice['ask_size1'] + EPSILON)\n",
    "    sum_bid_sizes = df_slice['bid_size1'] + df_slice['bid_size2']\n",
    "    sum_ask_sizes = df_slice['ask_size1'] + df_slice['ask_size2']\n",
    "    features['book_pressure'] = sum_bid_sizes / (sum_bid_sizes + sum_ask_sizes + EPSILON)\n",
    "    return features\n",
    "\n",
    "def calculate_time_id_features(df_group):\n",
    "    df_group = df_group.sort_values('seconds_in_bucket').copy() \n",
    "    snapshot_features = calculate_basic_features_snapshot(df_group)\n",
    "    log_returns = np.log(snapshot_features['micro_price'] / snapshot_features['micro_price'].shift(1))\n",
    "    log_returns = log_returns.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    results = {}\n",
    "    results['realized_volatility'] = np.std(log_returns) if len(log_returns) > 1 else 0\n",
    "    results['realized_skewness'] = skew(log_returns) if len(log_returns) > 1 else 0\n",
    "    if len(log_returns) > 2: \n",
    "        ac, _ = pearsonr(log_returns.iloc[1:], log_returns.iloc[:-1])\n",
    "        results['autocorrelation_log_returns'] = ac if not np.isnan(ac) else 0\n",
    "    else:\n",
    "        results['autocorrelation_log_returns'] = 0\n",
    "        \n",
    "    results['tick_frequency'] = len(df_group)\n",
    "    results['mean_micro_price'] = snapshot_features['micro_price'].mean()\n",
    "    results['mean_spread1'] = snapshot_features['spread1'].mean()\n",
    "    results['mean_spread2'] = snapshot_features['spread2'].mean()\n",
    "    results['mean_imbalance_size1'] = snapshot_features['imbalance_size1'].mean()\n",
    "    results['mean_book_pressure'] = snapshot_features['book_pressure'].mean()\n",
    "    results['mean_bid_size1'] = df_group['bid_size1'].mean()\n",
    "    results['mean_ask_size1'] = df_group['ask_size1'].mean()\n",
    "    results['mean_bid_size2'] = df_group['bid_size2'].mean()\n",
    "    results['mean_ask_size2'] = df_group['ask_size2'].mean()\n",
    "            \n",
    "    return pd.Series(results)\n",
    "\n",
    "def qlike_loss(y_true, y_pred):\n",
    "    y_pred = np.maximum(y_pred, EPSILON) \n",
    "    y_true = np.maximum(y_true, 0)      \n",
    "    valid_indices = (y_true > EPSILON) \n",
    "    if not np.any(valid_indices):\n",
    "        return np.nan \n",
    "    y_true_f = y_true[valid_indices]\n",
    "    y_pred_f = y_pred[valid_indices]\n",
    "    y_pred_f = np.maximum(y_pred_f, EPSILON)\n",
    "    loss = np.mean(y_true_f / y_pred_f - np.log(y_true_f / y_pred_f) - 1)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating features per stock_id and time_id...\n",
      "Calculated detailed features for 428932 stock/time_id pairs.\n",
      "   stock_id  time_id  realized_volatility  realized_skewness  \\\n",
      "0         0        5             0.000259           0.331051   \n",
      "1         0       11             0.000085          -1.095674   \n",
      "2         0       16             0.000173          -0.190956   \n",
      "3         0       31             0.000235          -1.939494   \n",
      "4         0       62             0.000143           0.450891   \n",
      "\n",
      "   autocorrelation_log_returns  tick_frequency  mean_micro_price  \\\n",
      "0                    -0.229365           302.0          1.003725   \n",
      "1                    -0.223872           200.0          1.000239   \n",
      "2                    -0.315257           188.0          0.999542   \n",
      "3                    -0.114241           120.0          0.998832   \n",
      "4                    -0.217718           176.0          0.999619   \n",
      "\n",
      "   mean_spread1  mean_spread2  mean_imbalance_size1  mean_book_pressure  \\\n",
      "0      0.000855      0.001181             -0.006357            0.506577   \n",
      "1      0.000394      0.000671              0.167165            0.588501   \n",
      "2      0.000725      0.001120             -0.105494            0.515192   \n",
      "3      0.000859      0.001157              0.056378            0.440347   \n",
      "4      0.000397      0.000697              0.088670            0.635624   \n",
      "\n",
      "   mean_bid_size1  mean_ask_size1  mean_bid_size2  mean_ask_size2  \n",
      "0       78.264901       74.579470       80.880795       89.771523  \n",
      "1      149.965000       71.145000       95.445000       94.895000  \n",
      "2       96.132979      131.037234      114.526596       74.654255  \n",
      "3      114.458333      120.800000       68.783333      131.225000  \n",
      "4      119.823864       88.477273       87.840909       47.079545  \n"
     ]
    }
   ],
   "source": [
    "print(\"Calculating features per stock_id and time_id...\")\n",
    "stock_time_id_features = df.groupby(['stock_id', 'time_id']).apply(calculate_time_id_features).reset_index()\n",
    "print(f\"Calculated detailed features for {stock_time_id_features.shape[0]} stock/time_id pairs.\")\n",
    "print(stock_time_id_features.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtering pathological stocks...\n",
      "Original number of stocks: 112\n",
      "Number of stocks after volatility filtering: 111\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFiltering pathological stocks...\")\n",
    "overall_stock_mean_rv = stock_time_id_features.groupby('stock_id')['realized_volatility'].mean().reset_index()\n",
    "overall_stock_mean_rv = overall_stock_mean_rv.rename(columns={'realized_volatility': 'mean_realized_volatility'})\n",
    "\n",
    "q1 = overall_stock_mean_rv['mean_realized_volatility'].quantile(0.25)\n",
    "q3 = overall_stock_mean_rv['mean_realized_volatility'].quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "\n",
    "lower_bound = q1 - VOLATILITY_IQR_MULTIPLIER * iqr\n",
    "upper_bound = q3 + VOLATILITY_IQR_MULTIPLIER * iqr\n",
    "\n",
    "epsilon_vol = 1e-7 \n",
    "filtered_stocks_info = overall_stock_mean_rv[\n",
    "    (overall_stock_mean_rv['mean_realized_volatility'] >= lower_bound) &\n",
    "    (overall_stock_mean_rv['mean_realized_volatility'] <= upper_bound) &\n",
    "    (overall_stock_mean_rv['mean_realized_volatility'] > epsilon_vol)\n",
    "]\n",
    "\n",
    "n_original_stocks = df['stock_id'].nunique()\n",
    "n_filtered_stocks = filtered_stocks_info['stock_id'].nunique()\n",
    "print(f\"Original number of stocks: {n_original_stocks}\")\n",
    "print(f\"Number of stocks after volatility filtering: {n_filtered_stocks}\")\n",
    "\n",
    "if n_filtered_stocks == 0:\n",
    "    print(\"Error: No stocks remaining after filtering. Adjust VOLATILITY_IQR_MULTIPLIER or check data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_time_id_features_filtered = stock_time_id_features[\n",
    "    stock_time_id_features['stock_id'].isin(filtered_stocks_info['stock_id'])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Engineering features for K-means clustering...\n",
      "Meta-features for clustering (mean of time_id features per stock):\n",
      "          realized_volatility  realized_skewness  autocorrelation_log_returns  \\\n",
      "stock_id                                                                        \n",
      "0                    0.000291          -0.029187                    -0.189381   \n",
      "1                    0.000242          -0.017519                    -0.154757   \n",
      "2                    0.000117          -0.048594                    -0.138243   \n",
      "3                    0.000374          -0.037155                    -0.173425   \n",
      "4                    0.000276           0.047078                    -0.191476   \n",
      "\n",
      "          tick_frequency  mean_micro_price  mean_spread1  mean_spread2  \\\n",
      "stock_id                                                                 \n",
      "0             239.569974          1.000072      0.001033      0.001432   \n",
      "1             393.611488          1.000007      0.000708      0.001061   \n",
      "2             496.903394          1.000019      0.000264      0.000496   \n",
      "3             331.451958          0.999934      0.001075      0.001579   \n",
      "4             280.414883          1.000102      0.000916      0.001195   \n",
      "\n",
      "          mean_imbalance_size1  mean_book_pressure  mean_bid_size1  \\\n",
      "stock_id                                                             \n",
      "0                     0.054838            0.521920      114.891325   \n",
      "1                    -0.014186            0.493764      139.409820   \n",
      "2                    -0.005281            0.496041      194.273333   \n",
      "3                    -0.038029            0.482145      131.163734   \n",
      "4                     0.023883            0.523341      102.756988   \n",
      "\n",
      "          mean_ask_size1  mean_bid_size2  mean_ask_size2  \n",
      "stock_id                                                  \n",
      "0             101.815896       87.238433       84.083882  \n",
      "1             142.114232      129.411910      133.414987  \n",
      "2             199.884532      216.494547      223.348143  \n",
      "3             140.111126      117.451351      130.868997  \n",
      "4              97.430189       83.245268       69.344429  \n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEngineering features for K-means clustering...\")\n",
    "\n",
    "cluster_feature_cols = [\n",
    "    'realized_volatility', 'realized_skewness', 'autocorrelation_log_returns', \n",
    "    'tick_frequency', 'mean_micro_price', 'mean_spread1', 'mean_spread2', \n",
    "    'mean_imbalance_size1', 'mean_book_pressure',\n",
    "    'mean_bid_size1', 'mean_ask_size1', 'mean_bid_size2', 'mean_ask_size2'\n",
    "]\n",
    "\n",
    "stock_meta_features_df = stock_time_id_features_filtered.groupby('stock_id')[cluster_feature_cols].mean()\n",
    "print(\"Meta-features for clustering (mean of time_id features per stock):\")\n",
    "print(stock_meta_features_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_meta_features = scaler.fit_transform(stock_meta_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing K-means clustering with K=5...\n",
      "Clustering results (stock_id and assigned cluster):\n",
      "          cluster\n",
      "stock_id         \n",
      "0               3\n",
      "1               2\n",
      "2               2\n",
      "3               4\n",
      "4               3\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nPerforming K-means clustering with K={N_CLUSTERS}...\")\n",
    "kmeans = KMeans(n_clusters=N_CLUSTERS, random_state=RANDOM_STATE, n_init='auto')\n",
    "stock_meta_features_df['cluster'] = kmeans.fit_predict(scaled_meta_features)\n",
    "print(\"Clustering results (stock_id and assigned cluster):\")\n",
    "print(stock_meta_features_df[['cluster']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating R-squared and QLIKE scores using expanding window approach...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCalculating R-squared and QLIKE scores using expanding window approach\")\n",
    "r_squared_feature_cols = [\n",
    "    'realized_volatility', 'mean_spread1', 'mean_imbalance_size1', \n",
    "    'mean_book_pressure', 'mean_micro_price'\n",
    "]\n",
    "stock_scores_list = []\n",
    "stock_time_id_features_filtered = stock_time_id_features_filtered.sort_values(['stock_id', 'time_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock 0: R^2 = -0.0167, QLIKE = 0.2002 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 1: R^2 = -0.0256, QLIKE = 0.1533 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 2: R^2 = -0.0286, QLIKE = 0.2506 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 3: R^2 = -0.2260, QLIKE = 0.1261 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 4: R^2 = -0.0602, QLIKE = 0.1870 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 5: R^2 = -0.0390, QLIKE = 0.1365 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 6: R^2 = -0.0922, QLIKE = 0.0973 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 7: R^2 = -0.0177, QLIKE = 0.1046 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 8: R^2 = -0.0135, QLIKE = 0.1477 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 9: R^2 = -0.0932, QLIKE = 0.1539 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 10: R^2 = -0.0263, QLIKE = 0.1909 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 11: R^2 = -0.0585, QLIKE = 0.1841 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 13: R^2 = -0.0534, QLIKE = 0.1992 (from 3823 R2 points, 3823 QLIKE points)\n",
      "Stock 14: R^2 = -0.0237, QLIKE = 0.1912 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 15: R^2 = -0.1003, QLIKE = 0.1864 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 16: R^2 = -0.0404, QLIKE = 0.1995 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 17: R^2 = -0.0196, QLIKE = 0.1672 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 19: R^2 = -0.0383, QLIKE = 0.2102 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 20: R^2 = -0.0371, QLIKE = 0.1821 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 21: R^2 = -0.0272, QLIKE = 0.1692 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 22: R^2 = -0.0125, QLIKE = 0.1568 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 23: R^2 = -0.0627, QLIKE = 0.1537 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 26: R^2 = -0.0283, QLIKE = 0.2005 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 27: R^2 = -0.0172, QLIKE = 0.1689 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 28: R^2 = -0.0196, QLIKE = 0.1803 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 29: R^2 = -0.6054, QLIKE = 0.1722 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 30: R^2 = -0.0242, QLIKE = 0.2278 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 31: R^2 = -0.0108, QLIKE = 0.1860 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 32: R^2 = -0.0124, QLIKE = 0.1673 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 33: R^2 = -0.0161, QLIKE = 0.1929 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 34: R^2 = -0.0391, QLIKE = 0.1796 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 35: R^2 = -0.0157, QLIKE = 0.1927 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 36: R^2 = -2.9604, QLIKE = 0.1377 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 37: R^2 = -0.2598, QLIKE = 0.1389 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 38: R^2 = -0.0259, QLIKE = 0.1619 (from 3809 R2 points, 3809 QLIKE points)\n",
      "Stock 39: R^2 = -0.0355, QLIKE = 0.1970 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 40: R^2 = -28008.1092, QLIKE = 0.1413 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 41: R^2 = -0.1016, QLIKE = 0.1803 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 42: R^2 = -0.0188, QLIKE = 0.1754 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 43: R^2 = -0.0125, QLIKE = 0.2229 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 44: R^2 = -0.0140, QLIKE = 0.1661 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 46: R^2 = -0.1036, QLIKE = 0.1727 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 47: R^2 = -0.0817, QLIKE = 0.1824 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 48: R^2 = -0.2018, QLIKE = 0.1579 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 50: R^2 = -0.0125, QLIKE = 0.1232 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 51: R^2 = -0.6339, QLIKE = 0.2043 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 52: R^2 = -0.0207, QLIKE = 0.1846 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 53: R^2 = -0.0362, QLIKE = 0.1562 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 55: R^2 = -0.0171, QLIKE = 0.1414 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 56: R^2 = -0.1021, QLIKE = 0.1180 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 58: R^2 = -0.4730, QLIKE = 0.2017 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 59: R^2 = -0.0140, QLIKE = 0.1696 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 60: R^2 = -0.0464, QLIKE = 0.1848 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 61: R^2 = -0.6081, QLIKE = 0.0968 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 62: R^2 = -0.0342, QLIKE = 0.1447 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 63: R^2 = -0.0215, QLIKE = 0.1343 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 64: R^2 = -0.0340, QLIKE = 0.1634 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 66: R^2 = -0.0423, QLIKE = 0.1994 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 67: R^2 = -0.0214, QLIKE = 0.1847 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 68: R^2 = -0.0204, QLIKE = 0.2447 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 69: R^2 = -0.0177, QLIKE = 0.1726 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 70: R^2 = -0.0316, QLIKE = 0.2038 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 72: R^2 = -0.0565, QLIKE = 0.2184 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 73: R^2 = -0.0377, QLIKE = 0.1293 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 74: R^2 = -0.0243, QLIKE = 0.1741 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 75: R^2 = -0.0279, QLIKE = 0.1573 (from 3823 R2 points, 3823 QLIKE points)\n",
      "Stock 76: R^2 = -0.0158, QLIKE = 0.1532 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 77: R^2 = -0.0733, QLIKE = 0.1649 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 78: R^2 = -0.0144, QLIKE = 0.1474 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 80: R^2 = -0.0305, QLIKE = 0.1417 (from 3814 R2 points, 3814 QLIKE points)\n",
      "Stock 81: R^2 = -0.0093, QLIKE = 0.1220 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 82: R^2 = -0.0152, QLIKE = 0.1794 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 83: R^2 = -0.0544, QLIKE = 0.1770 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 84: R^2 = -0.0152, QLIKE = 0.1461 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 85: R^2 = -0.0108, QLIKE = 0.1668 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 86: R^2 = -0.0305, QLIKE = 0.1383 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 87: R^2 = -0.0587, QLIKE = 0.1592 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 88: R^2 = -0.0470, QLIKE = 0.1853 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 89: R^2 = -0.0128, QLIKE = 0.1687 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 90: R^2 = -0.0616, QLIKE = 0.1685 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 93: R^2 = -0.0541, QLIKE = 0.1920 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 94: R^2 = -0.0296, QLIKE = 0.1666 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 95: R^2 = -4.7017, QLIKE = 0.1614 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 96: R^2 = -0.0137, QLIKE = 0.1123 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 97: R^2 = -0.0371, QLIKE = 0.1319 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 98: R^2 = -0.0801, QLIKE = 0.1978 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 99: R^2 = -0.0165, QLIKE = 0.1900 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 100: R^2 = -0.0311, QLIKE = 0.1820 (from 3823 R2 points, 3823 QLIKE points)\n",
      "Stock 101: R^2 = -0.0300, QLIKE = 0.1415 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 102: R^2 = -0.0166, QLIKE = 0.1629 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 103: R^2 = -0.0409, QLIKE = 0.2002 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 104: R^2 = -0.0489, QLIKE = 0.1837 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 105: R^2 = -0.2983, QLIKE = 0.1862 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 107: R^2 = -0.0296, QLIKE = 0.2227 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 108: R^2 = -0.0336, QLIKE = 0.1961 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 109: R^2 = -0.0160, QLIKE = 0.1540 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 110: R^2 = -0.0774, QLIKE = 0.1102 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 111: R^2 = -0.0238, QLIKE = 0.1792 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 112: R^2 = -0.3217, QLIKE = 0.1777 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 113: R^2 = -0.7819, QLIKE = 0.1039 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 114: R^2 = -0.2205, QLIKE = 0.1703 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 115: R^2 = -0.0150, QLIKE = 0.1607 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 116: R^2 = -0.0194, QLIKE = 0.1683 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 118: R^2 = -0.1172, QLIKE = 0.1664 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 119: R^2 = -1.0446, QLIKE = 0.2198 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 120: R^2 = -0.0104, QLIKE = 0.1594 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 122: R^2 = -0.1426, QLIKE = 0.1849 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 123: R^2 = -0.0281, QLIKE = 0.1996 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 124: R^2 = -0.3443, QLIKE = 0.1347 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 125: R^2 = -0.0399, QLIKE = 0.1951 (from 3824 R2 points, 3824 QLIKE points)\n",
      "Stock 126: R^2 = -0.0383, QLIKE = 0.1541 (from 3824 R2 points, 3824 QLIKE points)\n"
     ]
    }
   ],
   "source": [
    "for stock_id in filtered_stocks_info['stock_id']:\n",
    "    stock_data = stock_time_id_features_filtered[stock_time_id_features_filtered['stock_id'] == stock_id].copy()\n",
    "    \n",
    "    if len(stock_data) < MIN_PERIODS_FOR_MODEL:\n",
    "        print(f\"Stock {stock_id}: Insufficient data ({len(stock_data)} periods) for R2/QLIKE, skipping.\")\n",
    "        stock_scores_list.append({'stock_id': stock_id, 'r_squared': np.nan, 'qlike': np.nan})\n",
    "        continue\n",
    "\n",
    "    for col in r_squared_feature_cols:\n",
    "        stock_data[f'prev_{col}'] = stock_data[col].shift(1)\n",
    "    \n",
    "    stock_data = stock_data.dropna() \n",
    "\n",
    "    if len(stock_data) < 2: \n",
    "        print(f\"Stock {stock_id}: Insufficient data after lagging for R2/QLIKE, skipping.\")\n",
    "        stock_scores_list.append({'stock_id': stock_id, 'r_squared': np.nan, 'qlike': np.nan})\n",
    "        continue\n",
    "\n",
    "    y_true_r2_all = []\n",
    "    y_pred_r2_all = []\n",
    "    y_true_qlike_all = []\n",
    "    y_pred_qlike_all = []\n",
    "\n",
    "    start_prediction_idx = max(2, MIN_PERIODS_FOR_MODEL // 2)\n",
    "\n",
    "\n",
    "    for i in range(start_prediction_idx, len(stock_data)):\n",
    "        train_df = stock_data.iloc[:i]\n",
    "        current_period_data = stock_data.iloc[i]\n",
    "\n",
    "        X_train = train_df[[f'prev_{col}' for col in r_squared_feature_cols]]\n",
    "        y_train = train_df['realized_volatility']\n",
    "        \n",
    "        X_current = pd.DataFrame(current_period_data[[f'prev_{col}' for col in r_squared_feature_cols]]).T\n",
    "        y_current_true_r2 = current_period_data['realized_volatility']\n",
    "\n",
    "        if len(X_train) >= 2: \n",
    "            try:\n",
    "                model = LinearRegression()\n",
    "                model.fit(X_train, y_train)\n",
    "                y_current_pred_r2 = model.predict(X_current)[0]\n",
    "                \n",
    "                y_true_r2_all.append(y_current_true_r2)\n",
    "                y_pred_r2_all.append(y_current_pred_r2)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        historical_rv_for_qlike = train_df['realized_volatility']\n",
    "        if not historical_rv_for_qlike.empty:\n",
    "            forecast_rv_qlike = historical_rv_for_qlike.mean()\n",
    "            y_current_true_qlike = current_period_data['realized_volatility']\n",
    "\n",
    "            y_true_qlike_all.append(y_current_true_qlike)\n",
    "            y_pred_qlike_all.append(forecast_rv_qlike)\n",
    "\n",
    "    r_squared_stock = np.nan\n",
    "    if len(y_true_r2_all) >= 2 and len(set(y_true_r2_all)) > 1: \n",
    "        r_squared_stock = r2_score(y_true_r2_all, y_pred_r2_all)\n",
    "    \n",
    "    qlike_stock = np.nan\n",
    "    if y_true_qlike_all:\n",
    "        qlike_stock = qlike_loss(np.array(y_true_qlike_all), np.array(y_pred_qlike_all))\n",
    "\n",
    "    stock_scores_list.append({\n",
    "        'stock_id': stock_id,\n",
    "        'r_squared': r_squared_stock,\n",
    "        'qlike': qlike_stock\n",
    "    })\n",
    "    print(f\"Stock {stock_id}: R^2 = {r_squared_stock:.4f}, QLIKE = {qlike_stock:.4f} (from {len(y_true_r2_all)} R2 points, {len(y_true_qlike_all)} QLIKE points)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculated R-squared and QLIKE scores:\n",
      "   stock_id  r_squared     qlike  cluster\n",
      "0         0  -0.016697  0.200236        3\n",
      "1         1  -0.025567  0.153346        2\n",
      "2         2  -0.028575  0.250551        2\n",
      "3         3  -0.225981  0.126084        4\n",
      "4         4  -0.060184  0.186953        3\n"
     ]
    }
   ],
   "source": [
    "stock_scores_df = pd.DataFrame(stock_scores_list)\n",
    "stock_scores_df = pd.merge(stock_scores_df, stock_meta_features_df[['cluster']].reset_index(), on='stock_id', how='left')\n",
    "print(\"\\nCalculated R-squared and QLIKE scores:\")\n",
    "print(stock_scores_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combining scores and selecting top stocks...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCombining scores and selecting top stocks...\")\n",
    "stock_scores_df = stock_scores_df.dropna(subset=['r_squared', 'qlike'])\n",
    "if stock_scores_df.empty:\n",
    "    print(\"Error: No stocks remaining after calculating R-squared/QLIKE (all NaNs or empty). Check calculation steps or MIN_PERIODS_FOR_MODEL.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 30 stocks based on combined score (0.5*R^2 - 0.5*QLIKE):\n",
      "     stock_id  r_squared     qlike  cluster  combined_score\n",
      "7           7  -0.017700  0.104596        2       -0.061148\n",
      "83         96  -0.013730  0.112310        2       -0.063020\n",
      "70         81  -0.009333  0.122045        1       -0.065689\n",
      "44         50  -0.012519  0.123174        0       -0.067847\n",
      "55         63  -0.021452  0.134259        2       -0.077856\n",
      "48         55  -0.017104  0.141370        4       -0.079237\n",
      "73         84  -0.015184  0.146063        2       -0.080624\n",
      "8           8  -0.013549  0.147736        2       -0.080642\n",
      "68         78  -0.014410  0.147396        4       -0.080903\n",
      "63         73  -0.037662  0.129327        1       -0.083494\n",
      "75         86  -0.030463  0.138283        1       -0.084373\n",
      "66         76  -0.015786  0.153200        2       -0.084493\n",
      "84         97  -0.037139  0.131908        4       -0.084523\n",
      "20         22  -0.012541  0.156816        4       -0.084679\n",
      "105       120  -0.010393  0.159367        0       -0.084880\n",
      "95        109  -0.016024  0.154014        1       -0.085019\n",
      "88        101  -0.030027  0.141496        2       -0.085761\n",
      "69         80  -0.030490  0.141663        4       -0.086077\n",
      "5           5  -0.039017  0.136491        4       -0.087754\n",
      "101       115  -0.015049  0.160729        1       -0.087889\n",
      "74         85  -0.010796  0.166832        1       -0.088814\n",
      "1           1  -0.025567  0.153346        2       -0.089456\n",
      "54         62  -0.034200  0.144738        4       -0.089469\n",
      "89        102  -0.016618  0.162935        4       -0.089776\n",
      "28         32  -0.012386  0.167294        0       -0.089840\n",
      "40         44  -0.014012  0.166081        0       -0.090046\n",
      "78         89  -0.012787  0.168669        1       -0.090728\n",
      "51         59  -0.014043  0.169586        1       -0.091814\n",
      "65         75  -0.027921  0.157343        3       -0.092632\n",
      "23         27  -0.017169  0.168861        4       -0.093015\n"
     ]
    }
   ],
   "source": [
    "stock_scores_df['combined_score'] = R2_WEIGHT * stock_scores_df['r_squared'] - \\\n",
    "                                    QLIKE_WEIGHT * stock_scores_df['qlike']\n",
    "top_stocks = stock_scores_df.sort_values(by='combined_score', ascending=False)\n",
    "print(f\"\\nTop 30 stocks based on combined score ({R2_WEIGHT}*R^2 - {QLIKE_WEIGHT}*QLIKE):\")\n",
    "N_TOP_STOCKS = 30\n",
    "final_selection = top_stocks.head(N_TOP_STOCKS)\n",
    "print(final_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_stock_ids = [1, 5, 7, 8, 22, 27, 32, 44, 50, 55, \n",
    "                      59, 62, 63, 73, 75, 76, 78, 80, 81, 84, \n",
    "                      85, 86, 89, 96, 97, 101, 102, 109, 115, 120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"Data/112Stocks.parquet\")\n",
    "df = df[df[\"stock_id\"].isin(selected_stock_ids)]\n",
    "df.to_parquet(\"Data/30Stocks.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Feature Engineering and Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"Data/30stocks.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    df['mid_price'] = (df['bid_price1'] + df['ask_price1']) / 2\n",
    "    df['spread']    = df['ask_price1'] - df['bid_price1']\n",
    "    \n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        num  = df['bid_size1'] - df['ask_size1']\n",
    "        den  = df['bid_size1'] + df['ask_size1']\n",
    "        df['imbalance'] = np.where(den > 0, num / den, np.nan)\n",
    "\n",
    "        num2 = (df['bid_size1'] + df['bid_size2']) - (df['ask_size1'] + df['ask_size2'])\n",
    "        den2 = df[['bid_size1','bid_size2','ask_size1','ask_size2']].sum(axis=1)\n",
    "        df['book_pressure'] = np.where(den2 > 0, num2 / den2, np.nan)\n",
    "\n",
    "    df['normalized_spread'] = df['spread'] / df['mid_price'].replace(0, np.nan)\n",
    "    df['OBI_L2'] = np.where(den2 > 0, (df['bid_size1'] + df['bid_size2']) / den2, np.nan)\n",
    "\n",
    "    sizes = df[['bid_size1','bid_size2','ask_size1','ask_size2']].astype(float).values\n",
    "    total = sizes.sum(axis=1, keepdims=True)\n",
    "    p = np.divide(sizes, total, where=total != 0)\n",
    "    entropy = -np.nansum(np.where(p > 0, p * np.log(p), 0), axis=1)\n",
    "    df['LOB_entropy'] = entropy\n",
    "    df['LOB_entropy_normalized'] = entropy / np.log(4)\n",
    "\n",
    "    df['log_return'] = (\n",
    "        df.groupby('time_id')['mid_price']\n",
    "          .transform(lambda x: np.log(x / x.shift(1)))\n",
    "    )\n",
    "\n",
    "    df['realized_volatility'] = (\n",
    "        df.groupby('time_id')['log_return']\n",
    "        .transform(lambda x: np.sqrt(\n",
    "            ((x.shift(1) ** 2)\n",
    "                .rolling(30, min_periods=1)\n",
    "                .sum()\n",
    "            ).clip(lower=0)\n",
    "        ))\n",
    "    )\n",
    "\n",
    "    df['rv_future'] = (\n",
    "        df.groupby('time_id')['realized_volatility'].shift(-30)   \n",
    "    )\n",
    "\n",
    "    df['bipower_var'] = (\n",
    "        df.groupby('time_id')['log_return']\n",
    "          .transform(lambda x: x.abs().shift(1)\n",
    "                       .rolling(2, min_periods=1)\n",
    "                       .apply(lambda r: r[0] * r[1], raw=True)\n",
    "                       .rolling(30, min_periods=1)\n",
    "                       .mean())\n",
    "    )\n",
    "\n",
    "    df['wap'] = (\n",
    "        (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) /\n",
    "        (df['bid_size1'] + df['ask_size1']).replace(0, np.nan)\n",
    "    )\n",
    "    \n",
    "    df['log_wap_return'] = (\n",
    "        df.groupby('time_id')['wap']\n",
    "          .transform(lambda x: np.log(x / x.shift(1)))\n",
    "    )\n",
    "\n",
    "    for col in ['imbalance', 'book_pressure', 'log_return']:\n",
    "        df[f'{col}_lag1'] = df.groupby('time_id')[col].shift(1)\n",
    "        df[f'{col}_lag2'] = df.groupby('time_id')[col].shift(2)\n",
    "\n",
    "    df['rolling_vol_30'] = (\n",
    "        df.groupby('time_id')['log_return']\n",
    "          .transform(lambda x: x.shift(1).rolling(30, min_periods=1).std())\n",
    "    )\n",
    "    df['rolling_imbalance_mean_30'] = (\n",
    "        df.groupby('time_id')['imbalance']\n",
    "          .transform(lambda x: x.shift(1).rolling(30, min_periods=1).mean())\n",
    "    )\n",
    "\n",
    "    df = df.dropna()   \n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    theta = 2 * np.pi * df['seconds_in_bucket'] / 600 # period = 600\n",
    "    df['sec_sin'] = np.sin(theta)\n",
    "    df['sec_cos'] = np.cos(theta)\n",
    "\n",
    "    for c in ['bid_size1','ask_size1','bid_size2','ask_size2']:\n",
    "        df[c + '_log'] = np.log1p(df[c])\n",
    "        df.drop(columns=c, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = make_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.astype({col: 'float32' if df[col].dtype == 'float64' else 'int32' \n",
    "                for col in df.columns \n",
    "                if df[col].dtype in ['float64', 'int64']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['rv_future'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = VarianceThreshold(threshold=0.0)\n",
    "X_reduced = selector.fit_transform(X)\n",
    "selected_columns = X.columns[selector.get_support()]\n",
    "X_reduced_df = pd.DataFrame(X_reduced, columns=selected_columns, index=X.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfR = X_reduced_df.astype({col: 'float32' if X_reduced_df[col].dtype == 'float64' else 'int32' \n",
    "                for col in X_reduced_df.columns \n",
    "                if X_reduced_df[col].dtype in ['float64', 'int64']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LOB_entropy_normalized',\n",
       " 'OBI_L2',\n",
       " 'ask_price1',\n",
       " 'ask_price2',\n",
       " 'bid_price1',\n",
       " 'bid_price2',\n",
       " 'normalized_spread',\n",
       " 'realized_volatility',\n",
       " 'wap'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr = dfR.corr(method='spearman').abs()\n",
    "to_drop = {c for c in corr.columns for r in corr.columns\n",
    "if r != c and corr.loc[r, c] > .98 and corr.loc[r].sum() < corr.loc[c].sum()}\n",
    "to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfR = dfR.drop(columns=list(to_drop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfR['rv_future'] = df['rv_future']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfR = dfR.sort_values(\n",
    "    ['stock_id', 'time_id', 'seconds_in_bucket'],\n",
    "    ascending=[True, True, True]\n",
    ").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfR.to_parquet(\"Data/FE30Stocks.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model 1: WLS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qlike_loss(actual, pred, eps=1e-12):\n",
    "    a = np.clip(actual,   eps, None)\n",
    "    f = np.clip(pred, eps, None)\n",
    "    r = a / f\n",
    "    return np.mean(r - np.log(r) - 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['stock_id','mid_price', 'spread', 'imbalance',\n",
    "       'book_pressure', 'LOB_entropy', 'log_return', 'bipower_var',\n",
    "       'log_wap_return', 'imbalance_lag1', 'imbalance_lag2',\n",
    "       'book_pressure_lag1', 'book_pressure_lag2', 'log_return_lag1',\n",
    "       'log_return_lag2', 'rolling_vol_30', 'rolling_imbalance_mean_30',\n",
    "       'sec_sin', 'sec_cos', 'bid_size1_log', 'ask_size1_log', 'bid_size2_log',\n",
    "       'ask_size2_log']\n",
    "target_col = 'rv_future'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"/Users/ayush/Documents/University/Year 03/Sem 01/DATA3888/Optiver-07/Data/FE30Stocks.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[feature_cols].astype('float32')\n",
    "y = df[target_col].astype('float32')\n",
    "w = 1.0 / (y.rolling(2000, min_periods=1).var().fillna(y.var()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_idx = int(len(df) * 0.8)        \n",
    "X_train, X_test = X.iloc[:split_idx],  X.iloc[split_idx:]\n",
    "y_train, y_test = y.iloc[:split_idx],  y.iloc[split_idx:]\n",
    "w_train, w_test = w.iloc[:split_idx],  w.iloc[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_c = sm.add_constant(X_train, has_constant='add')\n",
    "X_test_c  = sm.add_constant(X_test,  has_constant='add')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model     = sm.WLS(y_train, X_train_c, weights=w_train)\n",
    "results   = model.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = results.predict(X_test_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2     = r2_score(y_test, y_pred)\n",
    "qlike  = qlike_loss(y_test.values, y_pred)\n",
    "print(f\"Out-of-sample R²   : {r2:0.4f}\")\n",
    "print(f\"Out-of-sample QLIKE: {qlike:0.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model 2: Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"/Users/ayush/Documents/University/Year 03/Sem 01/DATA3888/Optiver-07/Data/FE30Stocks.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols_mod = ['stock_id', 'mid_price', 'spread', 'imbalance',\n",
    "       'book_pressure', 'LOB_entropy', 'log_return', 'bipower_var',\n",
    "       'log_wap_return', 'imbalance_lag1', 'imbalance_lag2',\n",
    "       'book_pressure_lag1', 'book_pressure_lag2', 'log_return_lag1',\n",
    "       'log_return_lag2', 'rolling_vol_30', 'rolling_imbalance_mean_30',\n",
    "       'sec_sin', 'sec_cos', 'bid_size1_log', 'ask_size1_log', 'bid_size2_log',\n",
    "       'ask_size2_log']\n",
    "target_col = \"rv_future\"\n",
    "\n",
    "df['rv_future_log'] = np.log1p(df[target_col])\n",
    "target_col_mod   = 'rv_future_log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_sessions = np.sort(df['time_id'].unique())\n",
    "split_idx       = int(len(unique_sessions) * 0.8)\n",
    "\n",
    "train_val_sessions = unique_sessions[:split_idx]\n",
    "test_sessions      = unique_sessions[split_idx:]\n",
    "\n",
    "train_val_df = df[df['time_id'].isin(train_val_sessions)].copy()\n",
    "test_df      = df[df['time_id'].isin(test_sessions)].copy()\n",
    "\n",
    "val_cut        = int(len(train_val_sessions) * 0.9)\n",
    "train_sessions = train_val_sessions[:val_cut]\n",
    "val_sessions   = train_val_sessions[val_cut:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_val_df[train_val_df['time_id'].isin(train_sessions)]\n",
    "val_df   = train_val_df[train_val_df['time_id'].isin(val_sessions)]\n",
    "\n",
    "X_train = train_df[feature_cols_mod].values\n",
    "y_train = train_df[target_col_mod].values.ravel()\n",
    "\n",
    "X_val   = val_df[feature_cols_mod].values\n",
    "y_val   = val_df[target_col_mod].values.ravel()\n",
    "\n",
    "X_test  = test_df[feature_cols_mod].values\n",
    "y_test  = test_df[target_col_mod].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(\n",
    "    n_estimators=500,\n",
    "    max_depth=None,\n",
    "    max_features='sqrt',\n",
    "    min_samples_leaf=3,\n",
    "    bootstrap=True,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred  = rf.predict(X_val)\n",
    "val_rmse  = root_mean_squared_error(y_val, val_pred)\n",
    "print(f\"Validation RMSE: {val_rmse:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = rf.predict(X_test)\n",
    "rmse = root_mean_squared_error(y_test, pred)\n",
    "print(f\"Out-of-sample RMSE = {rmse:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_raw = y_test                  \n",
    "y_pred_raw = rf.predict(X_test)     \n",
    "\n",
    "rmse = root_mean_squared_error(y_true_raw, y_pred_raw)\n",
    "r2 = r2_score(y_true_raw, y_pred_raw)\n",
    "\n",
    "def qlike_safe(actual, forecast, eps=1e-8):\n",
    "    a = np.clip(actual, eps, None)\n",
    "    f = np.clip(forecast, eps, None)\n",
    "    r = a / f\n",
    "    return np.mean(r - np.log(r) - 1.0)\n",
    "\n",
    "ql = qlike_safe(y_true_raw, y_pred_raw)\n",
    "\n",
    "print(f\"Out‑of‑sample RMSE: {rmse:.6f}\")\n",
    "print(f\"R² score         : {r2:.6f}\")\n",
    "print(f\"QLIKE            : {ql:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(y_test, pred, s=6, alpha=0.6, edgecolor=\"none\")\n",
    "max_val = max(np.max(y_test), np.max(pred))\n",
    "plt.plot([0, max_val], [0, max_val], linestyle=\"--\")\n",
    "plt.title(\"Predicted vs. realised volatility\")\n",
    "plt.xlabel(\"True σ\")\n",
    "plt.ylabel(\"Predicted σ\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = pred - y_test\n",
    "plt.figure()\n",
    "plt.hist(residuals, bins=60, alpha=0.8)\n",
    "plt.title(\"Residual distribution (prediction - truth)\")\n",
    "plt.xlabel(\"Error\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model 3: LSTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(3888)\n",
    "np.random.seed(3888)\n",
    "tf.random.set_seed(3888)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"/Users/ayush/Documents/University/Year 03/Sem 01/DATA3888/Optiver-07/Data/FE30Stocks.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['mid_price', 'spread',\n",
    "       'imbalance', 'book_pressure', 'LOB_entropy', 'log_return',\n",
    "       'bipower_var', 'log_wap_return', 'imbalance_lag1', 'imbalance_lag2',\n",
    "       'book_pressure_lag1', 'book_pressure_lag2', 'log_return_lag1',\n",
    "       'log_return_lag2', 'rolling_vol_30', 'rolling_imbalance_mean_30',\n",
    "       'sec_sin', 'sec_cos', 'bid_size1_log', 'ask_size1_log', 'bid_size2_log',\n",
    "       'ask_size2_log']\n",
    "target_col = \"rv_future\"\n",
    "df['rv_future_log'] = np.log1p(df['rv_future'])\n",
    "feature_cols_mod = [c for c in feature_cols if c!='rv_future'] \n",
    "target_col_mod     = 'rv_future_log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_sessions = df[\"time_id\"].sort_values().unique()\n",
    "split_idx       = int(len(unique_sessions) * 0.8)          \n",
    "train_sessions  = unique_sessions[:split_idx]\n",
    "test_sessions   = unique_sessions[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[df[\"time_id\"].isin(train_sessions)].copy()\n",
    "test_df  = df[df[\"time_id\"].isin(test_sessions)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scaler = MinMaxScaler().fit(train_df[feature_cols_mod])\n",
    "y_scaler = MinMaxScaler(feature_range=(0,1)).fit(train_df[[target_col_mod]])\n",
    "\n",
    "train_df[feature_cols_mod] = x_scaler.transform(train_df[feature_cols_mod])\n",
    "test_df[feature_cols_mod]  = x_scaler.transform(test_df[feature_cols_mod])\n",
    "train_df[target_col_mod]   = y_scaler.transform(train_df[[target_col_mod]])\n",
    "test_df[target_col_mod]    = y_scaler.transform(test_df[[target_col_mod]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sequences(df_part: pd.DataFrame, feature_cols, target_col, seq_len):\n",
    "    X, y = [], []\n",
    "    for _, session in df_part.groupby(\"time_id\"):\n",
    "        data   = session[feature_cols].values\n",
    "        target = session[target_col].values\n",
    "        for i in range(len(session) - seq_len):\n",
    "            X.append(data[i : i + seq_len])\n",
    "            y.append(target[i + seq_len])\n",
    "    return np.asarray(X), np.asarray(y)\n",
    "\n",
    "X_train, y_train = build_sequences(train_df, feature_cols, target_col, SEQ_LEN)\n",
    "X_test,  y_test  = build_sequences(test_df,  feature_cols, target_col, SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_split_idx = int(len(train_sessions) * 0.9)\n",
    "val_sessions  = train_sessions[val_split_idx:]\n",
    "train_sessions= train_sessions[:val_split_idx]\n",
    "\n",
    "val_df = train_df[train_df[\"time_id\"].isin(val_sessions)]\n",
    "train_df = train_df[train_df[\"time_id\"].isin(train_sessions)]\n",
    "\n",
    "X_train, y_train = build_sequences(train_df, feature_cols_mod, 'rv_future_log', SEQ_LEN)\n",
    "X_val,   y_val   = build_sequences(val_df,   feature_cols_mod, 'rv_future_log', SEQ_LEN)\n",
    "X_test,  y_test  = build_sequences(test_df,  feature_cols_mod, 'rv_future_log', SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model(seq_len, num_features):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, return_sequences=True, input_shape=(seq_len, num_features)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(32))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(1))  \n",
    "    return model\n",
    "\n",
    "NUM_FEATURES = X_train.shape[2]\n",
    "model = build_lstm_model(SEQ_LEN, NUM_FEATURES)\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "]\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss='mse',\n",
    "    metrics=['mae', 'mse']\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50, \n",
    "    batch_size=128,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_scaled = model.predict(X_test, verbose=1).flatten()\n",
    "actual_scaled = y_test.flatten()\n",
    "predictions = y_scaler.inverse_transform(pred_scaled.reshape(-1, 1)).flatten()\n",
    "actuals     = y_scaler.inverse_transform(actual_scaled.reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ql = qlike_loss(actuals, predictions)\n",
    "mse   = np.mean((predictions - actuals) ** 2)\n",
    "rmse  = np.sqrt(mse)\n",
    "r2 = r2_score(actuals, predictions)\n",
    "print(f\"Test RMSE (volatility): {rmse:.9f}\")\n",
    "print(f\"R² score (σ prediction): {r2:.6f}\")\n",
    "print(\"QLIKE:\", ql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(actuals, predictions, alpha=0.3, s=8)\n",
    "lim = [min(actuals.min(), predictions.min()),\n",
    "       max(actuals.max(), predictions.max())]\n",
    "plt.plot(lim, lim)                     \n",
    "plt.title(f\"All test samples   $R^2$ = {r2_score(actuals, predictions):.3f}\")\n",
    "plt.xlabel(\"Actual volatility\")\n",
    "plt.ylabel(\"Predicted volatility\")\n",
    "plt.gca().set_aspect(\"equal\", adjustable=\"box\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title('LSTM Loss vs. Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model 4: Transformer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"/Users/ayush/Documents/University/Year 03/Sem 01/DATA3888/Optiver-07/Data/FE30Stocks.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['mid_price', 'spread',\n",
    "       'imbalance', 'book_pressure', 'LOB_entropy', 'log_return',\n",
    "       'bipower_var', 'log_wap_return', 'imbalance_lag1', 'imbalance_lag2',\n",
    "       'book_pressure_lag1', 'book_pressure_lag2', 'log_return_lag1',\n",
    "       'log_return_lag2', 'rolling_vol_30', 'rolling_imbalance_mean_30',\n",
    "       'sec_sin', 'sec_cos', 'bid_size1_log', 'ask_size1_log', 'bid_size2_log',\n",
    "       'ask_size2_log']\n",
    "target_col = \"rv_future\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rv_future_log'] = np.log1p(df['rv_future'])\n",
    "feature_cols_mod = [c for c in feature_cols if c!='rv_future'] \n",
    "target_col_mod     = 'rv_future_log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_sessions = df[\"time_id\"].sort_values().unique()\n",
    "split_idx       = int(len(unique_sessions) * 0.8)          \n",
    "train_sessions  = unique_sessions[:split_idx]\n",
    "test_sessions   = unique_sessions[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[df[\"time_id\"].isin(train_sessions)].copy()\n",
    "test_df  = df[df[\"time_id\"].isin(test_sessions)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scaler = MinMaxScaler().fit(train_df[feature_cols_mod])\n",
    "y_scaler = MinMaxScaler(feature_range=(0,1)).fit(train_df[[target_col_mod]])\n",
    "\n",
    "train_df[feature_cols_mod] = x_scaler.transform(train_df[feature_cols_mod])\n",
    "test_df[feature_cols_mod]  = x_scaler.transform(test_df[feature_cols_mod])\n",
    "train_df[target_col_mod]   = y_scaler.transform(train_df[[target_col_mod]])\n",
    "test_df[target_col_mod]    = y_scaler.transform(test_df[[target_col_mod]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = build_sequences(train_df, feature_cols, target_col, SEQ_LEN)\n",
    "X_test,  y_test  = build_sequences(test_df,  feature_cols, target_col, SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer_model(seq_len, num_features, d_model=64, num_heads=4, num_layers=2):\n",
    "    inputs = layers.Input(shape=(seq_len, num_features))\n",
    "    x = layers.Dense(d_model)(inputs)\n",
    "    for _ in range(num_layers):\n",
    "        attn_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=d_model)(x, x)\n",
    "        x = layers.Add()([x, attn_output])\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "        ffn_out = layers.Dense(d_model * 4, activation=\"relu\")(x)\n",
    "        ffn_out = layers.Dense(d_model)(ffn_out)\n",
    "        x       = layers.Add()([x, ffn_out])\n",
    "        x       = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)  \n",
    "    output = layers.Dense(1)(x)\n",
    "    return models.Model(inputs, output)\n",
    "\n",
    "model = build_transformer_model(SEQ_LEN, len(feature_cols))\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-3), loss=\"mse\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_split_idx = int(len(train_sessions) * 0.9)\n",
    "val_sessions  = train_sessions[val_split_idx:]\n",
    "train_sessions= train_sessions[:val_split_idx]\n",
    "\n",
    "val_df = train_df[train_df[\"time_id\"].isin(val_sessions)]\n",
    "train_df = train_df[train_df[\"time_id\"].isin(train_sessions)]\n",
    "\n",
    "X_train, y_train = build_sequences(train_df, feature_cols_mod, 'rv_future_log', SEQ_LEN)\n",
    "X_val,   y_val   = build_sequences(val_df,   feature_cols_mod, 'rv_future_log', SEQ_LEN)\n",
    "X_test,  y_test  = build_sequences(test_df,  feature_cols_mod, 'rv_future_log', SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_feats = X_train.shape[2]  \n",
    "model = build_transformer_model(SEQ_LEN, num_feats)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "    loss=\"mse\"\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=15, restore_best_weights=True\n",
    "    )],\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_scaled = model.predict(X_test, verbose=1).flatten()\n",
    "actual_scaled = y_test.flatten()\n",
    "predictions = y_scaler.inverse_transform(pred_scaled.reshape(-1, 1)).flatten()\n",
    "actuals     = y_scaler.inverse_transform(actual_scaled.reshape(-1, 1)).flatten()\n",
    "mse   = np.mean((predictions - actuals) ** 2)\n",
    "rmse  = np.sqrt(mse)\n",
    "print(f\"Test RMSE (volatility): {rmse:.9f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = r2_score(actuals, predictions)\n",
    "print(f\"R² score (σ prediction) = {r2:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qlike_safe(actual, forecast, eps=1e-12):\n",
    "    a = np.clip(actual,   eps, None)\n",
    "    f = np.clip(forecast, eps, None)\n",
    "    r = a / f\n",
    "    return np.mean(r - np.log(r) - 1.0)\n",
    "\n",
    "ql = qlike_safe(actuals, predictions)\n",
    "print(\"QLIKE:\", ql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 6))\n",
    "plt.plot(actuals, label=\"True Volatility\")\n",
    "plt.plot(predictions, label=\"Predicted Volatility\")\n",
    "plt.legend()\n",
    "plt.title(\"Transformer — Volatility Prediction\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(actuals, predictions, alpha=0.3, s=8)\n",
    "lim = [min(actuals.min(), predictions.min()),\n",
    "       max(actuals.max(), predictions.max())]\n",
    "plt.plot(lim, lim)                     \n",
    "plt.title(f\"All test samples   $R^2$ = {r2_score(actuals, predictions):.3f}\")\n",
    "plt.xlabel(\"Actual volatility\")\n",
    "plt.ylabel(\"Predicted volatility\")\n",
    "plt.gca().set_aspect(\"equal\", adjustable=\"box\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = predictions - actuals\n",
    "plt.figure()\n",
    "plt.hist(residuals, bins=100)\n",
    "plt.title(\"Residual distribution (Pred - Actual)\")\n",
    "plt.xlabel(\"Residual\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "volt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
