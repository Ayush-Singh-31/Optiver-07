{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DATA3888 Project: Optiver**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import dask.dataframe as dd\n",
    "import polars as pl\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = sorted(glob(\"Data/individual_book_train/*.csv\"))\n",
    "\n",
    "ldf = pl.scan_csv(\n",
    "    csv_files,\n",
    "    schema_overrides={         \n",
    "        'time_id': pl.Int64,\n",
    "        'seconds_in_bucket': pl.Int64,\n",
    "        'bid_price1': pl.Float64,\n",
    "        'ask_price1': pl.Float64,\n",
    "        'bid_price2': pl.Float64,\n",
    "        'ask_price2': pl.Float64,\n",
    "        'bid_size1': pl.Int64,\n",
    "        'ask_size1': pl.Int64,\n",
    "        'bid_size2': pl.Int64,\n",
    "        'ask_size2': pl.Int64,\n",
    "        'stock_id': pl.Int64,\n",
    "    },\n",
    "    infer_schema_length=0\n",
    ")\n",
    "\n",
    "df = ldf.collect()  \n",
    "df.write_parquet(\"Data/Combined_book_train.parquet\", compression=\"snappy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_parquet(\n",
    "    \"Data/Combined_book_train.parquet\",\n",
    "    use_pyarrow=True  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 1. Feature builder ─ all vectorised Polars expressions\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "class MakeFeaturesPolars:\n",
    "    def __init__(self, window: int = 30) -> None:\n",
    "        self.window = window            # rolling window length\n",
    "\n",
    "    # sklearn compatibility\n",
    "    def fit(self, df: pl.DataFrame, y=None): \n",
    "        return self\n",
    "\n",
    "    def transform(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        w       = self.window\n",
    "        sizes   = [\"bid_size1\", \"bid_size2\", \"ask_size1\", \"ask_size2\"]\n",
    "\n",
    "        out = (\n",
    "            df.sort([\"time_id\", \"seconds_in_bucket\"])      # guarantee order\n",
    "            # ───────────────────── basic microstructure columns ───────────────────\n",
    "              .with_columns([\n",
    "                  ((pl.col(\"bid_price1\") + pl.col(\"ask_price1\")) / 2)        .alias(\"mid_price\"),\n",
    "                  (pl.col(\"ask_price1\") - pl.col(\"bid_price1\"))              .alias(\"spread\")\n",
    "              ])\n",
    "              .with_columns([\n",
    "                  (pl.col(\"spread\") / pl.col(\"mid_price\"))                   .alias(\"rel_spread\"),\n",
    "                  ((pl.col(\"bid_size1\") - pl.col(\"ask_size1\")) /\n",
    "                   (pl.col(\"bid_size1\") + pl.col(\"ask_size1\")))              .alias(\"imbalance\"),\n",
    "                  (((pl.col(\"bid_size1\") + pl.col(\"bid_size2\")) -\n",
    "                    (pl.col(\"ask_size1\") + pl.col(\"ask_size2\"))) /\n",
    "                   pl.sum_horizontal(sizes))                                 .alias(\"book_pressure\"),\n",
    "                  (((pl.col(\"ask_price1\") * pl.col(\"bid_size1\") +\n",
    "                     pl.col(\"bid_price1\") * pl.col(\"ask_size1\")) ) /\n",
    "                   (pl.col(\"bid_size1\") + pl.col(\"ask_size1\")))              .alias(\"microprice\"),\n",
    "                  # keep if you still need it; identical to rel_spread\n",
    "                  pl.col(\"rel_spread\").alias(\"normalized_spread\"),\n",
    "                  ((pl.col(\"bid_size1\") + pl.col(\"bid_size2\")) /\n",
    "                   pl.sum_horizontal(sizes))                                 .alias(\"OBI_L2\"),\n",
    "              ])\n",
    "            # ───────────────────── LOB entropy (row-wise list math) ───────────────\n",
    "              .with_columns([\n",
    "                  (\n",
    "                      pl.concat_list(sizes)                                       # list of 4 elements\n",
    "                        .list.eval(pl.element() / pl.sum())                       # p_i\n",
    "                        .list.eval(                                               # –Σ p log p\n",
    "                            pl.when(pl.element() > 0)\n",
    "                              .then(pl.element() * pl.element().log())\n",
    "                              .otherwise(0.0),\n",
    "                            parallel=True)\n",
    "                        .list.sum()\n",
    "                        .map_batches(lambda s: -s)\n",
    "                        .alias(\"LOB_entropy\")\n",
    "                  )\n",
    "              ])\n",
    "              .with_columns(\n",
    "                  (pl.col(\"LOB_entropy\") / np.log(4)).alias(\"LOB_entropy_normalized\")\n",
    "              )\n",
    "            # ───────────────────── returns & realised-vol measures ───────────────\n",
    "              .with_columns([\n",
    "                  (pl.col(\"mid_price\").log().diff().over(\"time_id\"))              .alias(\"log_return\"),\n",
    "              ])\n",
    "              .with_columns([\n",
    "                  (pl.col(\"log_return\").pow(2)\n",
    "                       .rolling_sum(w, min_periods=1)\n",
    "                       .sqrt()\n",
    "                       .over(\"time_id\"))                                          .alias(\"realized_volatility\"),\n",
    "                  ((pl.col(\"log_return\").abs() *\n",
    "                    pl.col(\"log_return\").abs().shift(1))\n",
    "                       .rolling_mean(w, min_periods=1)\n",
    "                       .over(\"time_id\"))                                          .alias(\"bipower_var\"),\n",
    "                  (pl.col(\"log_return\").pow(2)\n",
    "                       .rolling_sum(w, min_periods=1)\n",
    "                       .over(\"time_id\"))                                          .alias(\"rolling_integrated_variance\")\n",
    "              ])\n",
    "              .drop_nulls()                             # tidy up\n",
    "        )\n",
    "        return out\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 2. Re-index & forward-fill each 10-minute window to 0…n-1 seconds\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "class ReindexFillPolars:\n",
    "    def __init__(self, n_seconds: int = 600) -> None:\n",
    "        self.n_seconds = n_seconds\n",
    "\n",
    "    def fit(self, df: pl.DataFrame, y=None): \n",
    "        return self\n",
    "\n",
    "    def transform(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        # all distinct time_ids\n",
    "        times  = df.select(\"time_id\").unique()\n",
    "        # seconds 0…n-1\n",
    "        secs   = pl.DataFrame({\"seconds_in_bucket\": range(self.n_seconds)})\n",
    "        # Cartesian product = full grid\n",
    "        full   = times.join(secs, how=\"cross\")\n",
    "        # left join original data, sort, forward-fill within each window\n",
    "        out = (\n",
    "            full\n",
    "            .join(df, on=[\"time_id\", \"seconds_in_bucket\"], how=\"left\")\n",
    "            .sort([\"time_id\", \"seconds_in_bucket\"])\n",
    "            .with_columns(pl.all().fill_null(strategy=\"forward\").over(\"time_id\"))\n",
    "        )\n",
    "        return out\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 3. MapTimeID – optional stock_id reconciliation + dense re-indexing\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "class MapTimeIDPolars:\n",
    "    def fit(self, df: pl.DataFrame, y=None): \n",
    "        return self\n",
    "\n",
    "    def transform(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        out = df.clone()\n",
    "\n",
    "        # 3 a. fill missing stock_id (if present) via one-to-one mapping\n",
    "        if \"stock_id\" in out.columns:\n",
    "            stock_map = (\n",
    "                out.drop_nulls(subset=[\"stock_id\"])\n",
    "                   .unique(subset=[\"time_id\"])\n",
    "                   .select([\"time_id\", \"stock_id\"])\n",
    "            )\n",
    "            out = (\n",
    "                out.drop(\"stock_id\")\n",
    "                   .join(stock_map, on=\"time_id\", how=\"left\")\n",
    "                   .drop_nulls()\n",
    "                   .drop(\"stock_id\")                       # mirror original logic\n",
    "            )\n",
    "\n",
    "        # 3 b. compress time_id to {1,2,…}\n",
    "        tids = out.select(\"time_id\").unique().sort(\"time_id\").to_series().to_list()\n",
    "        mapping = {old: new for new, old in enumerate(tids, start=1)}\n",
    "        out = out.with_columns(pl.col(\"time_id\").map_dict(mapping))\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 4. Convenience runner (same order as your original Pipeline)\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "def polars_pipeline(df: pl.DataFrame,\n",
    "                    n_seconds: int = 600,\n",
    "                    window: int = 30) -> pl.DataFrame:\n",
    "    df = MakeFeaturesPolars(window).transform(df)\n",
    "    df = ReindexFillPolars(n_seconds).transform(df)\n",
    "    df = MapTimeIDPolars().transform(df)\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "volt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
